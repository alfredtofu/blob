<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Face Recognition]]></title>
    <url>%2Fposts%2F2015%2F10%2F19%2FFace-Recognition%2F</url>
    <content type="text"><![CDATA[好久没写博客了。 最近做了快一个月的人脸识别问题，也算有点经验了。首先放出几个觉得真的有用的paper（至少我能验证有效的）： Deep Learning Face Representation by Joint Identification-Verification A Unified Embedding for Face Recognition and Clustering Bayesian Face Revisited: A Joint Formulation 趁着Deep Learning的浪潮，用它来做人脸识别的文章很多，比如DeepID/DeepID2/DeepID2+，DeepFace，FaceNet，Deep Face Recognition(VGG)，大家的效果都做的很不错，DeepFace不好实现，没3D alignment，其他的都还好。前三个是cuhk做的，FaceNet是google做的，Deep Face Recognition是VGG做的，后两个都是训练一个end to end的网络，区别在于VGG还会引入softmax（不同的人为不同的类别，训练一个多元分类器）。 DeepID2DeepID2比较复杂，大概的训练流程如下： 先训练200个网络（每一个网络都会有两个loss，一个是identification一个是verification），每个网络产生160维特征，每个网络都是同样的结构，但是输入不一样，区别在于： 人脸占图片的比例不一样 会以不同的关键点为图像中心 图像可以是彩图也可以是灰度图 左右翻转（这个我觉得应该不会额外训练网络，而是为同一个网络增加训练数据） 然后选择比较有效的25个网络出来，就可以得到25 * 160 = 4000维特征， 对4k特征做PCA得到180维特征， 对这180维特征做Joint Bayesian。 FaceNet &amp;&amp; VGG总的来说，这两个工作很类似，都是用triplet loss，而DeepID2是用Contrastive Loss，而我认为两种形式在一定程度上是等价的（两个pair能转化成triplet）。 FaceNet跟VGG的区别在于： FaceNet有很多很多的数据， FaceNet没有用softmax， FaceNet加上了hard mining， FaceNet的features加上了L2 Norm。 实验我做实验的时候，大体的框架是：多网络+L2Norm+triplet loss，多网络的features直接拼起来用两个全连接层再训练一次triplet。由于我是用我之前landmark的工作来对齐人脸的，那个工作只有5个点，所以每个尺度我只产生了5个网络，然后我用了两个尺度，所以总共有10个网络，目前我在LFW上是做到98.7167%。 下面是简单的记录一下我的实验结论(好像不止这些，心血来潮的时候写这篇博客，记得多少写多少吧)，face-0.5表示以人脸中心作为图像的中心且眼距占输入图像的0.5。 多网络多尺度是否有效单个：face-0.5是0.951667，face-0.3是 0.958167，联合是：0.9640联合后加PCA（95% eigenvalues，158 / 320）：0.9652联合后加PCA再加Joint Bayesian：0.9673 不同的点为中心：单个：face-0.5是0.951667，left-0.3是0.946667，联合是：0.9605联合后加PCA（95% eigenvalues，134 / 320）：0.9622联合后加PCA再加Joint Bayesian：0.9655 测试的时候加flipface-0.5是0.951667，加了flip之后是0.9565flip的加法是：对于A和B两张图片，A1跟A2表示图片A flip前后的照片，对应特征为fa1跟fa2那么最终两张图片的diff为 |fa1-fb1|^2 + |fa1-fb2|^2 + |fa2-fb1|^2 + |fa2-fb2|^2若是直接|fa1-fb1|^2 + |fa2-fb2|^2的准确率是0.956167 FC代替Joint BayesianJoint Bayesian的公式很漂亮，但是毕竟是个生成模型，并不能像判别性模型那样极大的为目标服务，而且用Joint Bayesian也会有一些问题，比如： 维度一高很难收敛，需要先用PCA降维，但是PCA对维度也很敏感，究竟要降多少维呢？反正每次我用不同维度，结果都不一样。 Flip的图像后的特征很难统一到模型里面，只能分开算。 此外我的实验结果也表明用两个全连接层就可以代替Joint Bayesian。 pair vs triplettriplet收敛很快，快很多，迟点上图。 分类作用没用！不知道是不是我调网络不好，确实真没用。有朋友也尝试过，发现没用。 此外，假设我有2W个人，像DeepID2这么简单的网络怎么对2w类进行分类？不现实。后面的所有的实验我都不加分类了。FaceNet就很明智的没加了。 Hard Mining有用！训练大网络（ZF-net）亲测有效，可以加快收敛。不过我后来也发现你训着训着，剩下不符合margin的基本都是很hard的，所以我后面也不加了… 学习率啊，不说什么了，loss稳定降学习率，所有网络都必须有一个点的提升。 L2 Norm好用！加快收敛之余让你训练更加方便，不容易发散。 数据–数据–数据太重要了。。。。。。。。。。。。。。。。。。。谁有更多的数据欢迎共享，MegaFace的数据看起来很难的样子。 （我感觉是未完待续，不过我感觉自己会懒得来更新了）]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>Face Recognition</tag>
        <tag>Face Verification</tag>
        <tag>Face Identification</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM for multi-target detection]]></title>
    <url>%2Fposts%2F2015%2F06%2F29%2FLSTM-for-multi-target-detection%2F</url>
    <content type="text"><![CDATA[论文题目是End-to-end people detection in crowded scenes，链接 截止目前，这是我看过的年度最佳论文，可以比肩LeCun做pose的paper，比R-CNN不知道高到哪里去，应该是NIPS2015的paper，你问我支不支持这个paper，我是支持的，我就明确告诉你，我还觉得可以是oral。 这是一个全新的多物体检测的框架（恕我愚昧），也很赶时髦，CNN+LSTM+end to end。以前的物体检测方法大体可以分为两种：sliding window fashion跟基于proposal的分类。像Overfeat就是sliding window fashion一种典型例子，它充分利用了卷积神经网络里面可以共享权值的特性；R-CNN呢，就是基于proposal的分类的例子，流程是这样的：产生proposal –&gt; 对proposal进行分类，分类对的就认为是最终检测的结果。想R-CNN这类方法有没有有点呢？有！直观，而且不用考虑尺度的问题，但是缺点也很明显，很难处理物体overlap比较大以及遮挡比较严重的情况，一般的proposal方法都不能产生这样的proposal。 回到这篇paper，前面提到他是一个CNN+LSTM的框架，结构图如下所示，CNN用来学习特征，然后LSTM用来产生预测结果。LSTM一个RNN，R字代表recurrent，循环的意思，也正是因为这样，RNN可以产生不固定长度的序列，把这个应用在多物体检测上真是绝配，毕竟多物体检测就是不确定有多少个检测目标。 在实际操作中，作者用了GoogLeNet作为特征的提取器，然后LSTM是基于GoogLeNet最顶层的特征(1024维)来进行预测。每一次，LSTM把1024维的特征以及前一个LSTM单元的输出作为下一个LSTM单元输入，每个LSTM单元的输出是一个五元组，用来表示一个检测结果，其中包括这个检测结果的分数以及预测框，当检测结果的分数低于某个阈值的时候，LSTM就停止forward。 然后Loss的定义就比较中规中矩，如下所示。 作者是首先将预测结果跟groundtruth做一次匹配，每个groundtruth只能跟一个预测结果匹配上，然后只对于匹配上的预测结果会惩罚它的预测框，所有的预测分数都会做一次惩罚。 最后还是感叹一下，这个思路真的很好，把多物体检测跟LSTM完美结合在一起，嵌入到CNN里面。]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>CNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[导数/偏导数/方向导数/梯度]]></title>
    <url>%2Fposts%2F2015%2F06%2F27%2Fgradient-descent%2F</url>
    <content type="text"><![CDATA[梯度下降法是求解神经网络的方法中最流行的一个，思想很简单，就是函数沿着梯度的方向下降的最快。通常来讲，我们在求解机器学习问题的时候，都会定义一个目标函数，然后基于这个目标函数又定义出损失函数，通过最小化损失函数来使得目标函数达到最优。那么在最小化损失函数的时候就可以用上梯度下降了。 思想简单，实现也很简单。在这篇文章里面，我主要是想讲讲梯度这个东西，因为我经常会被这个概念搞糊涂掉。梯度大一高数(记忆中高中也是讲过…)有教。什么是梯度？首先，它是一个向量，那向量肯定会有方向嘛，梯度的方向呢是使得方向导数达到最大值的方向，它的模就是方向导数的最大值。那什么是方向导数？理解这个东西需要知道导数这个概念，下面我打算从数学定义来说明导数/偏导数/方向导数/梯度这四个东西。 导数这个我想没有人会不知道。 定义：当函数$y=f(x)$的自变量在一点$x_0$上产生一个增量$\Delta x$时，函数输出值的增量与自变量增量$\Delta x$的比值在$\Delta x$趋于$0$时的极限如果存在，即为$f(x)$在$x_0$处的导数，记作$f'(x_0)$、$\frac{\mathrm{d}f}{\mathrm{d}x}(x_0)$或$\left.\frac{\mathrm{d}f}{\mathrm{d}x}\right|_{x=x_0}$ 我们从小到大对导数的认识就是认为它是函数曲线在相应点的切线的斜率。 偏导数慢慢的，我们当然不能仅限于一元函数的情况，那么多元函数求导是一个怎么概念？在多元函数情况下，通常我们计算的是偏导数，那么什么是偏导数？简单来讲，就是函数只对某个变量求导得到的导数就是函数关于这个变量的偏导数。比方说，对于二元函数$z = f(x, y)$，当我们把$y$固定住，然后对$x$求导，那么得到的导数称为$f(x, y)$对$x$的偏导数，记为$\frac{\mathrm{d}z}{\mathrm{d}x}$。同样的，固定$x$，对$y$求导得到的便是对$y$的偏导数，记为$\frac{\mathrm{d}z}{\mathrm{d}y}$。 方向导数那么实际上，偏导数$\frac{\mathrm{d}z}{\mathrm{d}x}$跟$\frac{\mathrm{d}z}{\mathrm{d}y}$是函数沿着相应的坐标轴方向的变化率(因为每次我们都固定住其它的变量)，那么如果需要同时考虑其他方向的变化率，该怎么办？这就引申出方向导数的概念了。 定义：设$z = f(x, y)$在一点$P_0(x_0, y_0)$的一个邻域内有定义，又设$\overrightarrow{l}$是给定的一个方向，其方向余弦为$(cos \alpha, cos \beta)$，若极限$$\begin{equation} \lim_{t \to 0}{\frac{f(x_0 + tcos \alpha, y_0 + tcos \beta) - f(x_0, y_0)}{t}} \end{equation}$$存在，则称此极限值为函数$z = f(x, y)$在$P_0$点沿方向$\overrightarrow{l}$的方向导数，记为$\left.\frac{\mathrm{d}z}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0}$ 实际上，$\left.\frac{\mathrm{d}z}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0}$是等于$\left.\frac{\mathrm{d}z}{\mathrm{d}x}\right|_{x_0, y_0} cos \alpha + \left.\frac{\mathrm{d}z}{\mathrm{d}y}\right|_{x_0, y_0} cos \beta$ 其中$t$为点$P_0(x_0, y_0)$到点$P_t(x_0 + tcos \alpha, y_0 + tcos \beta)$的距离。 很容易推广到多元的情况，比如三元函数$u = f(x, y, z)$在点$P_0(x_0, y_0, z_0)$沿方向$\overrightarrow{l}$的方向导数如下面是式子，$\overrightarrow{l}$方向的方向余弦为$(cos \alpha, cos \beta, cos \gamma)$ $$\begin{equation} \left.\frac{\mathrm{d}u}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0, z_0} = \left.\frac{\mathrm{d}u}{\mathrm{d}x}\right|_{x_0, y_0, z_0}cos \alpha + \left.\frac{\mathrm{d}u}{\mathrm{d}y}\right|_{x_0, y_0, z_0}cos \beta + \left.\frac{\mathrm{d}u}{\mathrm{d}y}\right|_{x_0, y_0, z_0}cos \gamma \end{equation}$$ 梯度因为引入了方向导数，那么自然而然就会想，在同一点上的所有方向导数中是否有最大值？显然是有的，而且这个方向导数就是梯度了！ 假设令$\overrightarrow{g} = (\left.\frac{\mathrm{d}z}{\mathrm{d}x}\right|_{x_0, y_0}, \left.\frac{\mathrm{d}z}{\mathrm{d}y}\right|_{x_0, y_0})$，那么就有 $$\begin{aligned} \left.\frac{\mathrm{d}z}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0} & = \overrightarrow{g} \cdot \overrightarrow{l} \\ & = | \overrightarrow{g} | | \overrightarrow{l} | cos \langle \overrightarrow{g}, \overrightarrow{l} \rangle \\ & = | \overrightarrow{g} | cos \langle \overrightarrow{g}, \overrightarrow{l} \rangle \end{aligned}$$ 显然，当$\langle \overrightarrow{g}, \overrightarrow{l} \rangle = 0$的时候，也就是$\overrightarrow{g} \textrm{跟} \overrightarrow{l}$同向的时候，函数关于$\overrightarrow{l}$的方向导数最大，且最大值为$| \overrightarrow{g} |$。 这里，$\overrightarrow{g} = (\left.\frac{\mathrm{d}z}{\mathrm{d}x}\right|_{x_0, y_0}, \left.\frac{\mathrm{d}z}{\mathrm{d}y}\right|_{x_0, y_0})$就成为函数在点$P_0(x_0, y_0)$的梯度，记为$\textrm{grad}\ z |_{x_0, y_0}$。 梯度下降法也是从这里来，当我们在求解的时候没办法直接得到最优解，只能不断逼近最优解，而逼近的时候又想尽可能的快，那么只能沿着函数变化最剧烈的方法(也就是梯度方向)的反方向走。这里顺便提一下梯度上升法，其实是跟梯度下降法一样，只是它是用于求解函数的最大值，在逼近最优解的时候，沿着梯度方向走。]]></content>
      <categories>
        <category>Academic</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Notes for Faster R-CNN and YOLO]]></title>
    <url>%2Fposts%2F2015%2F06%2F10%2FNotes-for-Faster-R-CNN-and-YOLO%2F</url>
    <content type="text"><![CDATA[最近NIPS 2015的投稿截止了，各种投到NIPS的文章也外泄了，看到了两篇关于Detection的文章，说实话，看到这些论文的时候一点兴奋都没有。 两篇paper都是出自rbg大神的: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks以及You Only Look Once: Unified, Real-Time Object Detection。然后两篇都是讲提速的，共同点都是把Selective Search(一些额外的proposal方法)去掉，用网络本身产生，做成一个end-to-end的框架。 Faster R-CNN说白了是一个能够联合优化的Cascade结构。 核心思想是引入了一个Region Proposal Networks(RPNs)，这个网络产生的proposals作为Fast R-CNN的输入。在特征提取上RPNs跟Fast R-CNN共享前面的所有卷积层，流程可简化如下： 其中RPNs的结构如下: 实际上操作的时候，会用RPNs在最后一层的特征图(Feature Map)上做划窗(Sliding window)，窗口的大小是$n \textrm{x} n$，然后每个窗口经过RPNs后会产生$k$个proposals，为什么不是一个而是使用$k$个？看文章的意思是为了得到平移/尺度不变形，不过在我看来是没必要的，这样给人感觉太过hand-crafted了，直接回归一个bbox也就可以了。作者好像没有对$k$做对比实验。 然后具体是怎么联合优化的？实际上，一开始就直接把整个网络进行联合优化是不合理的，因为proposal都不准，训练Fast R-CNN的分类网络意义何在….所以作者就提出了自己的训练方法，一开始，RPNs跟Fast R-CNN的卷积层是不共享的： 用ImageNet-pre-trained(AlexNet或VGG)来初始化RPN，然后对RPN进行fine-tune。 用ImageNet-pre-trained(AlexNet或VGG)来初始化Fast R-CNN，然后对Fast R-CNN进行fine-tune，其中这时候Fast R-CNN的proposals输入是由训练好的RPN产生的。 用Fast R-CNN来初始化RPN，固定卷积层，fine-tune全连接层。 固定卷积层，同时fine-tune RPN跟Fast R-CNN。 方法不太优美，但是又好像只能这么做。 YOLO这个很暴力… 只是大概讲一下思路，首先看一下YOLO的模型： 我想你应该没想错，就是这么暴力，对于一张图片，不管多大，都是先resize成448x448，然后扔进网络里面，网络就输出7x7=49个bbox，同时会为每个bbox输出它属于某种物体的概率。 那么具体这个7x7跟原图有什么关系以及groundtruth应该怎么设？作者是把原图划分成7x7=49个格子，然后如果原图上的某个物体的中点落在某个格子$b_{xy}$上，那么这个格子就负责预测这个物体出来，比如狗的bbox的中点落在(1, 5)这个格子，那么这个格子对应的groundtruth就是狗的bbox，以及狗的概率为1。 这有一个很严重的缺陷是，它没办法处理物体挨在一起且中点都落在同一个格子上的情况。]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[从SVN到Git，代码的合并]]></title>
    <url>%2Fposts%2F2015%2F06%2F04%2Fsvn-to-git%2F</url>
    <content type="text"><![CDATA[之前一直用的是旧版本的caffe，然后用svn管理，每次官方改了bug，都是手动加进去，很蛋疼…. 最近一怒之下转到git上了，那么问题来了，原来的版本我改了很多，怎么合并？以前没接触git，最多就是从github上clone项目下来，对merge啊，branch啊什么的完全没概念。折腾了一晚上终于搞掂了，期间大部分时间是花在手工修改代码上，下面讲讲我的经历。 将我的代码跟最新的caffe代码合并首先，先clone一份最新的caffe代码： 1git clone https://github.com/BVLC/caffe.git 然后新建一个空白分支，用来放我的代码 12git checkout --orphan zhujinrm -r * 然后把我的代码全拷贝进来，并做一次commit 12git add *git commit -m 'add my codes' 接着切换到master上，并进行合并： 12git checkout mastergit merge master zhujin 这时候一般是提示会有冲突，没办法，使用下面命令开始手动清理： 1git mergetool 清理完毕后，再看一下使用git status再看一下有没有冲突，如果没有，commit一次就完成了合并了。 通常弄完之后我会删除刚才新建的分支: 1git branch -d zhujin 创建本地仓库想必大家都不想这么快把自己的代码公开吧，毕竟有时候需要保密，这时候自己搭建一个git服务器是最安全而且方便的。那么先把现有仓库导出为裸仓库， 1git clone --bare caffe caffe.git 这样会生成一个叫caffe.git的文件夹，然后把它拷贝到你的服务器上，或者说本地的某个目录，这个文件夹就是仓库了，下次直接向这里进行clone/pull/push。我把这个文件夹放到了我的网盘上，直接clone就好： 1git clone /home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git 合并两个不同repository的分支现在遇到的问题是，因为我搭建了服务器，不能直接使用git pull从官方的repository更新了，这样很坑爹啊，如果官方修复了一些bug，那我还得手动的一个一个改回来。 然而，git还是很强大的，能够合并不同地址的两个分支。 首先，我本地服务器的git地址是/home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git，先进行clone 1git clone /home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git 这时候可以使用命令git remote -v查看你当前跟踪的repository有哪些，在我这里显示的只有我本地服务器的分支 12origin /home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git (fetch)origin /home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git (push) 此外，还可以用命令git branch -av看看目前的branch，显示： 123* master 8853960 fix bug, all test case pass except hdf5 remotes/origin/HEAD -&gt; origin/master remotes/origin/master 8853960 fix bug, all test case pass except hdf5 然后把caffe官方的repository也加进来，注意的是，BVLC是自己取的名字，用来区分不同的repository。 1git remote add BVLC https://github.com/BVLC/caffe.git 这时候使用命令git remote -v就发现你跟踪的repository多了。我这里显示的是： 1234BVLC https://github.com/BVLC/caffe.git (fetch)BVLC https://github.com/BVLC/caffe.git (push)origin /home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git (fetch)origin /home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git (push) 这时候需要使用命令git fetch BVLC从官网把最新的branch下载下来，如果有下面提示表示成功下载了。 1234567From https://github.com/BVLC/caffe * [new branch] dev -&gt; BVLC/dev * [new branch] device-abstraction -&gt; BVLC/device-abstraction * [new branch] gh-pages -&gt; BVLC/gh-pages * [new branch] master -&gt; BVLC/master * [new branch] parallel -&gt; BVLC/parallel * [new branch] tutorial -&gt; BVLC/tutorial 这时候再看一下分支，git branch -av：12345678910* master 8853960 fix bug, all test case pass except hdf5 remotes/BVLC/dev 9b0662f Merge pull request #1461 from ixartz/fix_convert_mnist_siamese_data remotes/BVLC/device-abstraction d52d669 Fix post-rebase errors, including: -device-abstracted version of MVN -new memset/memcpy wrappers (set_void and copy_void) -fixing MKL switching logic remotes/BVLC/gh-pages b7c55d7 ae4a5b1 Merge pull request #2505 from ronghanghu/matcaffe3 remotes/BVLC/master 72d7089 [bug] fix double instantiation of GPU methods in LogLayer remotes/BVLC/parallel aa3b877 Distributed training remotes/BVLC/tutorial e99d108 add tutorial link remotes/origin/HEAD -&gt; origin/master remotes/origin/master 8853960 fix bug, all test case pass except hdf5` 然后切换到本地的分支，并进行合并： 12git checkout mastergit merge BVLC/master 当然，合并前可以看看你的分支跟将要合并的分支的区别： 1git diff master BVLC/master merge完之后，还要更新到本地服务器:git push，OK，All is done. 更新代码好了，如果遇到caffe更新了代码怎么办？很简单，切换到BVLC/master分支，更新一下，然后切回master进行合并即可。 1234git checkout BVLC/mastergit pullgit checkout mastergit merge Update 搭建git服务器搭建git服务器也很简单，git是用ssh来访问的，用的是22端口，只要支持ssh访问就可以直接当做git服务器来用。那么得先安装OpenSSH服务: 1yum install openssh-server 这时候其实也不需要做什么了，git服务器已经可以用了…clone的地址变成: 1git clone liangzhujin@127.0.0.1:/home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git 上面，liangzhujin是用户名(这里的用户名是ssh访问时候用的用户名)，127.0.0.1是你git服务器的地址。 对了，如果你的仓库是放在家目录下的，那地址可以更简单，这里假设caffe.git就在/home/liangzhujin下: 1git clone liangzhujin@127.0.0.1:caffe.git 假设你需要用别的账号来使用git服务，而且这些账号是不能登陆shell的，比如我想新建一个用户git，这个账号只能用来使用git的服务器，而不能ssh登陆，那么方法也很简单: 新建用户1useradd git 禁用shell登陆修改/etc/passwd文件: 123git:x:118:126::/home/git:/bin/bash改成git:x:118:126::/home/git:/usr/bin/git-shell 修改仓库的拥有者假设仓库是caffe.git，需要把它的拥有者跟组设为git:1chown -R git:git caffe.git 然后把caffe.git放到/home/git/下。 进行clone这时候，就可以进行clone了:1git clone git@127.0.0.1:caffe.git 另外，如果想建立严格的权限控制，就要用到Gitolite Reference 服务器上的 Git - 在服务器上部署 Git Git 分支 - 分支的新建与合并]]></content>
      <categories>
        <category>折腾</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[The Role of Features, Algorithms and Data in Visual Recognition]]></title>
    <url>%2Fposts%2F2015%2F06%2F02%2FThe-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition%2F</url>
    <content type="text"><![CDATA[最近被华农的老师邀请去给她的研究生介绍一下Deep Learning，一时间没想到什么好的角度去讲。 看了不少大牛们关于Deep Learning的introduction，结合最近Deep Learning三大神牛Nature上的文章，还是发现了不少的东西，有些问题甚至自己都没想过的。 慢慢的就形成了做PPT的思路了，从特征工程引出特征学习，然后再从特征学习引出深度学习，然后再讲几个经典的深度模型，重点讲讲CNN。 update: slides做好了，提前放出来。下载链接: Pipeline of Pattern Recognition模式识别里面所有的任务都可以归结为下面一个流程： 正是因为这个流程，计算机视觉的发展都是围绕三个大的方面来进行： 数据。如何清洗数据？如何对数据做扰动，增加训练样本？等等等等。 特征表达。如何找到一种更有效的特征表达方式？抑或说怎么得到更加适合特定任务的特征，从而增加模型的准确性。 学习算法。怎么设计出一种高效准确的学习算法？ 毫无疑问的是，数据、特征表达以及学习算法都是非常重要的，不过哪个会比较重要？这么说或许不合理，应该说，在哪个方面下功夫对最终的结果影响最大？ Role of three factorsCVPR 2010年的一篇文章给出了线索，它做了大量的实验去比较跟验证哪个更重要，甚至还用做了人跟机器的性能比较，在这里我就不关心人的效果到底怎么样了。 The Role of Learning Algorithm首先比较了不同学习算法对结果的影响，实验方法是固定特征跟训练数据，使用不同的学习算法进行求解。下图就是实验结果，横轴是特征，纵轴是准确率，不同的子图表示用不同的数据集，不同颜色的曲线表示不同的学习算法。可以发现不同学习算法他们的效果会有区别，但是变化不会太大。 The Role of Data然后就是探究训练数据的不同对结果的影响，这时候就需要固定学习算法跟特征，使用不同的训练数据进行训练，下图是他们的实验结果，横轴是训练数据的数量，纵轴就是准确率，不同的子图表示不同的数据集以及不同的特征，不同颜色的曲线表示不同的学习算法，发现数据量增大的时候，效果提升还是挺明显的，然而到了一定量之后，数据增加，效果并不会提升了或者说提升非常慢。 The Role of Features最后就是探究特征的作用，这时候需要固定学习算法跟特征。下图是实验结果，横轴表示不同的学习算法，纵轴表示准确率，不同的子图表示不同数据集以及特征维度，不同颜色的曲线表示不同的特征，可以看出来，特征的选择对结果影响非常大。 通过这组对比实验，可以得出的结论是，特征很重要，特征一旦没选好，不管用什么算法，用多少数据量、用什么再强大的算法也没什么用，比如上面提到的红色的这条曲线，不管用多少数据，用什么学习算法，它的效果总是最烂的。相反，如果特征选得好，用再简单的算法也能达到相当不错的效果。 Feature Engineering那么这样就会引发我们的思考，特征是影响效果的最重要因素，然而在目前的主流系统里面，用的都是各种各样手工定义的特征，这些手工定义的特征有不少的缺陷： 很依赖于人的领域知识，那么比如如果一个学计算机视觉的人想要做车辆的检测，那么需要花很多时间去了解车辆，区分不同车辆之间的关系。 特征的设计跟学习算法是分开的，这样的一个坏处是，如果效果好，并不知道是特征好还是学习算法不好，难以分析。 如果设计的特征有很多参数，那么就很难进行调参。 如果出现了一个新的任务，那么设计有效的特征会非常缓慢。 比如下面这个图，我们想要学习出一张图片里面的深度信息，所谓的深度信息就是，物体距离我的距离，左边是一个正确的距离表示，不同颜色表示距离的远近，然而，这种任务的特征需要怎么设计？这非常困难，可以说是无从入手。 这时候自然而然就会想，能不能找到更有效的特征，能不能直接从数据里面学习出物体的特征，使得特征更适合我们需要解决的问题？ Feature Learning这当然是可以的，而且也一直被研究，叫特征学习。只不过在以前的时代，数据量不多，计算不够快，导致了这样的研究很难进行下去。 特征学习是一个比较久的概念了，它指的是建立分类器或者一些预测器的同时，学习出数据的一种转换，使得基于这种转换能够更容易的提取出有效的信息，为最终的任务服务，特征学习的方法的好处在于： 能够把分类器跟特征的表达一起学习，并做联合优化。 在这种方法底下，我们可以为特征表示设计出大量的参数并进行学习，这样可以极大的提高了深度模型的表达能力。通常来讲，参数越多，模型的表达能力会越强。 因为是直接从数据里面学习特征，那么会更有效的挖掘数据本身的特性。 对于新的应用领域，能够很快的得到一个比较好的特征表达方式。 那么有什么方法是可以用来学习特征的？ Deep Learning时下最火的Deep Learning就是一种非常强大的特征学习方法，他能够层次性的学习特征，高层的特征是用底层的特征表示的。这跟我们人的认知是很吻合的，像我们初中学的几何，我们就知道点动成线，线动成面，一样的道理，我们对一个物体的认识，通常都会把他进行拆解，比如人脸这个东西，通常我们理解是对他进行拆解成眼耳口鼻，然后不同的部分又是由不同的形状组合而成。右边这个图呢，是生物学的人对人脑的研究，发现脑对事物的认知是分层的，从简单到复杂。深度学习正是仿照这个脑认知来设计的，左边这个图是一个很好的说明，模型的输入直接是图像，然后一开始模型是学习出比较简单的特征，比如边缘，接着学习出更加复杂的特征，比如物体的部件，越往高层，特征越复杂，越接近我们想要了解的东西。 Deep Learning比较经典的模型有三个：DBN、Auto-Encoder以及CNN，这里主要讲一下CNN，对其他有兴趣的自行深入了解。 Convolutional Neural Networks (CNN)卷积神经网络，跟很多深度模型都不太一样，像DBN、Auto-Encoder等都是把数据拉成一个一维的向量处理，而CNN是把数据以多维的数组来处理，比如图像通常会表示为3个二维数组，每个二维数组表示一个颜色通道，RGB。 神经元在卷积层会被组织成特征图的形式，每一张特征图是一个二维数组，比如下图的第2~4个子图，都是特征图，在特征图里面，每个点只会跟前一层的某个小区域有关。这种连接关系会决定了参数的个数，比如假设特征图上的一个点跟前面的一个4x4的区域有连接，那么这时候就只会有16个参数，对于一个特征图上点来讲，他们都共享这些参数。这些参数我们通常就叫做滤波器，右下角就是把滤波器可视化后的结果。而pooling操作了是指将特征图相邻的神经元进行合并。 卷积神经网络里面有4个非常有技巧的设计，1) 局部连接， 2) 共享权重， 3) pooling，以及 4) 多层卷积的使用，它们都利用了自然信号的特性。我逐一讲解一下为什么这些东西是有效的。 对于局部连接，对于二维的数据，特别是图像，局部区域的值是高度相关的，而跟远离自己的区域的相关性很弱，所以不需要对太大的区域进行建模，这也是我们经常提到的图像局部区域相关性。在生物视觉的研究里面，当人注视前方的时候，只会关注前面的一小个区域，而不关注与周围的区域，按照这个理解，建模的时候不需要对太大的区域进行建模，需要对一小个区域建模就好了，这就是所谓的局部连接。 而对于共享权重，前面讲过，同一个特征图上的神经元是共享相同的参数的，理由是图像或者其他信号的局部统计信息是跟位置无关的，出现在哪里都是可以的，这有点抽象，举个例子，比如人脸的眼睛，它是可以出现在图像上的任意位置的，如果一个特征图是表示眼睛出现的概率，那么这个特征图上的神经元就自然而然的会共享同一组参数。 接下来是pooling，这个比较好理解，主要是用了降维，相邻的神经元表达的东西是很相似的，这时候需要对这些冗余的信息进行减少，加速计算速度 。 多个卷积层的使用，这是一个分层的概念了，多层主要是为了学习到更高层的语义特征。在卷积神经网络里面，底层的特征都是一些比较简单的特征，比如边缘，再往高层就是由边缘组成的部件，再往更高层就是由部件组成的物体了。 因为卷积神经网络有上面这些特性，导致了它的参数比普通的深度模型少很多，从而使得卷积神经网络比以前更加容易训练。所以最近很多工作都是基于卷积神经网络来做的。 以上。 Reference Deep Learning, Nature, 2015 The Role of Features, Algorithms and Data in Visual Recognition Deep Learning Tutorial, CVPR 2014 Introduction to Deep Learning, Xiaogang Wang]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Feature Engineering</tag>
        <tag>Feature Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Action Classification In Still Image (CNN篇)-2]]></title>
    <url>%2Fposts%2F2015%2F05%2F29%2Faction-still-image-cnn2%2F</url>
    <content type="text"><![CDATA[之前也写过几篇用CNN做静态图片的行为分类的论文笔记。最近又看了一些，继续写笔记。 Recognize Complex Events from Static Images by Fusing Deep Channelspaper链接 最近连续被Xiaoou刺激…Action的ideas都被做了，Video的idea被做，Still Image的也被做，大体的思路都一样，就是细节不一样，不过现在看来只能再想别的模型了。 这个paper说的是识别Event，这跟Action的分类是非常类似的，Action一般需要识别的是单个人，而Event关注的是一群人或者整张图片表示的事件，尽管如此，方法上还是非常像的。 文章核心想法是多信息融合。融合分两步，物体跟人的信息先进行一次融合(这能够对物体跟人的interaction进行建模)，融合后的特征再跟全局(全图)的信息融合，这种做法是很有道理的，物体跟人的交互往往是非常重要的线索(这么讲吧，如果能够很精确的检测物体的存在，这个Event是什么基本能够知道了，不过对于一些跟fine-grained的Event讲，交互/位置信息就很重要了，抱狗跟亲狗这两个Event，单单把狗检测出来还不够，还需要知道人头跟狗头的位置关系才能进一步区分)，然后全局的信息也是很重要的，我往往把这全局信息理解为场景信息，比如区分室内室外等等的。 用的依旧是CNN，既然是CNN的东西了，那么直接上网络结构就能懂了80%，网络结构如下： 接下来就根据不同的信息来讲吧，重点还是人跟物体这块，也就是网络结构图的下半部分。 全局信息这里直接上了AlexNet，看样子是取fc6出来做特征(4096维)，仍旧用ImageNet来pre-train，这没什么好讲了。 物体跟人因为数据集并没有给出人跟物体的位置，需要先对物体跟人的进行检测，检测出来之后，作者并没有直接把他们的坐标并起来，而是定义了一些特征，然后再让网络去学。 Feature for Human对于人的检测，作者并没有简单的用一个人的检测器，因为这很困难，同时如果人检测不到，那就谈不上interaction了，所以为了加强人的检测，作者还使用了人脸的检测，两者进行互补。(其实换成人头肩检测更好啊…) 那么如何构建人的特征？简单来讲，作者构建了binary maps，bboxes框住的地方的值为1，其他为0，下面是一个例子。 看了上面的图你也会发现，两个不同的Event，它们的特征几乎一样啊，这怎么区分？为了解决这个问题，作者就引入了多尺度的map，意思是，不同的大小的bboxes，会激活不同尺度的map，这个激活是指把bboxes框住的地方的值设为1。这里的不同大小的比较，是要在归一化的空间里面比较，归一化的方法是把原图resize成18x18，然后再算这时候bbox的面积。作者设置了两个阈值，这就划分出了3个空间，一个空间对应一个尺度，所以最终会得出3张map。人脸跟人都要做这样的操作，最终就会得出6张maps。例子例子： Feature for Object同样的方式构建特征。不过只有一个尺度。 作者进行了统计，得出了最常出现的30个物体，别问我为什么是30个….这时候就有30个maps了。 Fusion of Human and Object把6张maps跟30张maps拼起来就完成fusion了！接着就是架个CNN学特征了，看网络结构图即可，最终得到4096维的特征。 Fusion of All这里是对全局特征跟物体和人的特征进行融合，作者说是一种semantic fusion，将两者的4096维特征进行融合得到新的4096维特征，不过好像没说清楚怎么融合，加？乘？ Results因为没有现有的数据集，作者自己搞了个数据集，所以他实现了几个方法跟自己对比： 效果还是一如既往的不高，这个领域可提升空间真的很大。能看出CNN的威力了，作者的方法在Top 1对比其他CNN的方法提升不少，不过在Top 5区别就不大了。 最后说一句，最近都流行搞数据集了….我们实验室也是各种搞，这样有两个好处是，你可以很容易把别人PK掉，而且由于有数据集发布，你的引用也会增加不少。]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Action</tag>
        <tag>Still Image</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Centos 6.5搭建VPN服务器]]></title>
    <url>%2Fposts%2F2015%2F05%2F27%2Fset-up-vpn-in-centos-6-5%2F</url>
    <content type="text"><![CDATA[为什么要搭，都懂的…在电脑端主要是用ssh作为socks5代理(Windows用Bitvise SSH Client，Linux用ssh -N -D 7070 用户名@IP地址)，将http的报文封装在ssh的数据包里，在客户端和服务器之间传输，这个跟Chrome的插件SwitchyOmega(我的备份文件)搭配起来超级方便。不过手机上没办法用(iPhone得越狱，我没越狱)，只能再搭个VPN了。 首先检查需要搭建VPN的服务器有没有启用PPP以及TUN使用下面命令:12cat /dev/pppcat /dev/net/tun2 如果显示结果是:12cat: /dev/ppp: No such device or addresscat: /dev/net/tun: File descriptor in bad state 那么表示已经启用了。如果没启用，让你的服务提供商给你开吧。 安装必须要的软件需要用到ppp, iptables1yum install ppp iptables 添加pptpd的源并安装:12rpm -i http://poptop.sourceforge.net/yum/stable/rhel6/pptp-release-current.noarch.rpmyum install pptpd 配置pptp修改各种配置文件: /etc/pptpd.conf添加下面两行:12localip 192.168.0.1remoteip 192.168.0.10-111 192.168.0.10-111表示用户被分配的ip地址的范围。 /etc/ppp/options.pptpd添加下面两行，表示使用Google的DNS服务器:12ms-dns 8.8.8.8ms-dns 8.8.4.4 /etc/ppp/chap-secrets这个文件是用来添加vpn账号的，每一行表示一个用户，格式如下:1username pptpd passwd * username表示用户名 passwd表示账号 *表示任何ip，如果只想一个账号只能一个人登陆，就设置一个固定ip，参考上面的ip地址范围。我试过忘了填，导致vpn一直连不上，弄了很久才发现是这个问题… /etc/sysctl.conf修改内核设置，使得支持转发，将net.ipv4.ip_forward=0改为net.ipv4.ip_forward=1，要取消注释。修改后运行下面命令使得生效:1/sbin/sysctl -p 添加转发规则上面配置完了，重启pptpd服务后，应该是能登陆，但是没法上网，这时候需要添加iptables转发规则:1iptables -t nat -A POSTROUTING -s 192.168.0.0/24 -j SNAT --to-source server-ip 192.168.0.0/24: 这个是需要包括你上面分配的ip地址范围 vps-ip: 这里填的是你的服务器ip 保存iptables转发规则:1/etc/init.d/iptables save 然后重启iptables以及pptpd:12/etc/init.d/iptables restart/etc/init.d/pptpd restart 这时候就可以登陆并上网了。 设置开机启动把iptables以及pptpd都设置为开机启动:12chkconfig iptables onchkconfig pptpd on 客户端配置Google或者百度一下vpn pptp设置即可。 OK，搞掂。 我是很帅气的分割线 下面写个别的问题，就是解决用ssh代理无法登陆google的问题。会提示如下错误：123We're sorry...... but your computer or network may be sending automated queries. To protect our users, we can't process your request right now.See Google Help for more information. 百度搜了一下，是Google把linode的ipv6给屏蔽了，只需要把ipv6禁用了就可以了，方法如下:123echo '1' &gt; /proc/sys/net/ipv6/conf/lo/disable_ipv6echo '1' &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6echo '1' &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6]]></content>
      <categories>
        <category>折腾</category>
      </categories>
      <tags>
        <tag>Centos</tag>
        <tag>VPN</tag>
        <tag>PPTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在Centos 6.5配置FTP服务器]]></title>
    <url>%2Fposts%2F2015%2F05%2F27%2Fset-up-ftp-in-centos-6-5%2F</url>
    <content type="text"><![CDATA[因为要帮我哥写个软件，要用到存储服务，就自己搭了个FTP。在Centos 6.5下。主要是参考这个博客，后面根据自己的需求改了点东西。 安装vsftpd，ftp的服务器1yum install vsftpd 设置开机启动1chkconfig vsftpd on 启动服务1service vsftpd start 配置FTP用户组/用户以及相应权限添加用户组1groupadd ftp 添加用户1useradd -g ftp -M -d /srv/ftp/zhujin -s /sbin/nologin zhujin -g接的是用户组 -M表示不设置它的主目录，假设如果没有-M，则在/home下会有跟用户名(zhujin)一样的目录。 -d后面接的是用zhujin登陆FTP的时候，它的初始目录。 -s 后面接/sbin/nologin表示用户不需要登录系统，因为我们只需要用来登陆FTP zhujin表示用户名了 设置刚才添加的用户的密码1passwd zhujin 更改FTP目录的权限1chown -R zhujin:ftp /srv/ftp/zhujin 这时候重启vsftpd1/etc/init.d/vsftpd restart 把用户限制在固定的目录如果这时候登陆会发现刚才新建的用户可以访问并读取所有的目录的数据，这并不是我们想要的，需要把他们限定在某个目录下。修改配置文件vsftpd.conf，目录一般在/etc/vsftpd/vsftpd.conf，添加下面两行：12chroot_list_enable=YESchroot_list_file=/etc/vsftpd/chroot_list 然后在文件/etc/vsftpd/chroot_list里面填入你想要限制的用户，比如我就填入了zhujin，这时候重启vsftp，然后重新登陆就可以了。 设置匿名用户以及它的根目录允许匿名用户登陆需要修改配置文件vsftpd.conf，添加下面内容：1anonymous_enable=YES 设置匿名用户的根目录需要修改配置文件vsftpd.conf，添加下面内容：1anon_root=/srv/ftp/anon 完成后重启一下vsftpd1/etc/init.d/vsftpd restart 以上]]></content>
      <categories>
        <category>折腾</category>
      </categories>
      <tags>
        <tag>Centos</tag>
        <tag>vsftpd</tag>
        <tag>FTP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用Caffe时候遇到的问题(长期更新)]]></title>
    <url>%2Fposts%2F2015%2F05%2F21%2Fcaffe-problem%2F</url>
    <content type="text"><![CDATA[Caffe是一个深度学习的框架，本文主要是记录使用Caffe遇到的一些奇葩问题，不涉及对代码的理解。 Caffe的环境配置为了方便，我把我在Ubuntu 14.04下编译的依赖库上传到百度云，下载并修改Makefile.config即可编译Caffe，免去各种复杂的环境配置。在Ubuntu 12.04跟14.04都可用。 Anaconda的libm.so跟系统库的冲突提示的错误如下: 12345678910//usr/lib/x86_64-linux-gnu/libx264.so.142: undefined reference to `__exp_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libx264.so.142: undefined reference to `__log10_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libxvidcore.so.4: undefined reference to `__logf_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libvorbis.so.0: undefined reference to `__acosf_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libx264.so.142: undefined reference to `__pow_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libx264.so.142: undefined reference to `__log2_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libxvidcore.so.4: undefined reference to `__log10f_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libxvidcore.so.4: undefined reference to `__log_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libx264.so.142: undefined reference to `__powf_finite@GLIBC_2.15'//usr/lib/x86_64-linux-gnu/libx264.so.142: undefined reference to `__log2f_finite@GLIBC_2.15' 解决办法是把Anaconda的libm.so删掉或者改名，让Caffe编译的时候链接的是系统自带的。Anaconda的libm.so的路径是$HOME/anaconda/lib/libm.so]]></content>
      <tags>
        <tag>Caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Concepts and Tricks In CNN(长期更新)]]></title>
    <url>%2Fposts%2F2015%2F05%2F17%2Fcnn-trick%2F</url>
    <content type="text"><![CDATA[这篇文章主要讲一下Convolutional Neural Network(CNN)里面的一些概念以及技巧。 Receptive Field (感受野)这是一个非常重要的概念，receptive field往往是描述两个feature maps A/B上神经元的关系，假设从A经过若干个操作得到B，这时候B上的一个区域$\textrm{area}_b$只会跟a上的一个区域相关$\textrm{area}_a$，这时候$\textrm{area}_a$成为$\textrm{area}_b$的感受野。用图片来表示： 在上图里面，map 3里1x1的区域对应map 2的receptive field是那个红色的7x7的区域，而map 2里7x7的区域对应于map 1的receptive field是蓝色的11x11的区域，所以map 3里1x1的区域对应map 1的receptive field是蓝色的11x11的区域。 那么很容易得出来，receptive field的计算公式如下： 对于Convolution/Pooling layer: $$\begin{equation} r_i = s_i \cdot (r_{i + 1} - 1) + k_i \end{equation}$$ 其中$r_i$表示第$i$层layer的输入的某个区域，$s_i$表示第$i$层layer的步长，$k_i$表示kernel size，注意，不需要考虑padding size。 对于Neuron layer(ReLU/Sigmoid/…) $$\begin{equation} r_i = r_{i + 1} \end{equation}$$ Coordinate Mapping通常，我们需要知道网络里面任意两个feature map之间的坐标映射关系，如下图，我们想得到map 3上的点$p_3$映射回map 2所在的位置$p_2$。 计算公式如下： 对于Convolution/Pooling layer: $$\begin{equation} p_i = s_i \cdot p_{i + 1} + (\frac{k_i - 1}{2} - \textrm{padding}_i) \end{equation}$$ 其中$p_i$表示第$i$层layer的输入的某个点，$s_i$表示第$i$层layer的步长，$k_i$表示kernel size，$\textrm{padding}_i$ 对于Neuron layer(ReLU/Sigmoid/…) $$\begin{equation} p_i = p_{i + 1} \end{equation}$$ 上面是计算任意一个layer输入输出的坐标映射关系，如果是计算任意feature map之间的关系，只需要用简单的组合就可以得到，下图是一个简单的例子： Convolutionalize (卷积化)最近掀起了FCN(全卷积网络)风，这种网络里面不包括全连接层(fully connected layer)。 卷积层跟全连接层的区别卷积层的操作跟传统的滑窗(sliding windows)很相似，把kernel作用于输入的不同的区域然后产生对应的特征图，由于这样的性质，给定一个卷积层，它并不要求输入是固定大小的，它可能根据输入大小的不同而产生大小不一样的特征图。 全连接层的操作是把输入拉成一个一维的向量，然后对这一维的向量进行点乘，这就要求输入大小是固定的。 那么如果使用一个包含fc层的模型(如AlexNet)就必须使用固定大小的输入，其实有时候这是非常不方便以及不合理的，比如下图，如果我要把红框的塔输入网络，就必须得对它进行变成，假设是放到AlexNet里面，因为输入是224x224，那么就会对图片产生变形。 那么有没有办法使得网络可以接受任意的输入？实际上是可以的，只需要把全连接层变成卷积层，这就是所谓的卷积化。这里需要证明卷积化的等价性。直观上理解，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把卷积作用的区域扩大为整个输入，那就变成全连接了，我就不给出形式化定义了。所以我们只需要把卷积核变成跟输入的一个map的大小一样就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。举个例子，比如AlexNet，fc6的输入是256x6x6，那么这时候只需要把fc6变成是卷积核为6x6的卷积层就好了。 例子：(1) 用全连接的: full-connected.prototxt，(2) 改成全卷积：full-conv.prototxt]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Notes for "Object detection via a multi-region & semantic segmentation-aware CNN model"]]></title>
    <url>%2Fposts%2F2015%2F05%2F17%2Fmulti-region-semantic-segmentation-aware-cnn%2F</url>
    <content type="text"><![CDATA[paper链接 我觉得是一个挺有意思的paper，整体pipeline还是proposals –&gt; feature extraction –&gt; classification –&gt; post processing。 那么跟RCNN有啥区别？大体有两点：1) 不同的feature，作者加入了multi-region features以及semantic segmentation-aware features，2)不同的后处理过程，作者加入了迭代式定位以及bbox voting的方式。这个方法在PASCAL VOC2007的mAP是74.9%（RCNN是66%），在VOC2012是70.7%（RCNN是63%），相当大的提升啊。 下面就从feature extraction跟post processing来讲。 Features旨在找到更多具有判别性的特征。 Multi-Region CNN features对物体不同区域进行建模，得到具有更丰富表现性的特征。感觉就有点DPM的感觉了（不过是手动指定parts）。作者使用的区域有以下几种： Original candidate box (a): 这个就是proposal产生的区域了，用来得到物体整体的外观信息。 Half boxes (b/c/d/e): 把proposal按上下左右切半得到的区域。用来只学习一半物体的特征，可以用来处理遮挡情况。 Central Regions (f/g): 这部分的regions是用来学习物体中间部分的特征。 Border Regions (h/i): 这部分regions是用来学习物体边缘的特征，而且是joint来学。这部分特征对不精确定位的proposal的边缘特别敏感，因为不精确定位，比如过小，会导致丢失了边缘的信息。不过话说回来，这跟Half boxes不是有些矛盾么？ Contextual Region (j): 这种region是用来学习物体周围的信息。 最外围黄色的框表示实际上截取出来的region，上面的regions有些是Rectangular rings(矩形圈): g/h/i/j，在这些矩形圈里面，内圈的信息是我们不想要的，所以实际上在截取regions的时候，要简单做个后处理，就是对内圈的信息全部置为0。 这时候很多人就会想着把regions截取出来直接扔到CNN里面训练就好了，但是这会使得预算量比RCNN多了很多，速度会超级超级慢。所以作者在截取regions出来之后并不是直接用这些regions来训练，而是用它们在全图的feature maps上对应的位置截取相应的regions来训练(这里有讲)。这种方式有几个好处是：1) 节省计算，2) 减少参数，能起到正则化的作用，3) 能一起finetune，互相约束，我在做ICCV的时候也使用了这样的想法，果然在CNN里面大家的想法都是类似的，理论性不强，局限性很强，比的就是谁先做出来了，谁的机器多，谁的数据多。 那么就得出了下面的网络结构: 网络就可以分为两部分：activation maps module和region adaptation modules，region adaptation modules用了hekaiming的spatial pooling layer，能够防止了RCNN里面的强制变形，也能在一起程度上处理multi-scale（后面有时间会写一个RCNN/SPP-NET/Fast RCNN的对比）。 网络用的是VGG，在训练的时候，所有convolution layers的参数都是固定的，每个region modules各自finetune所有的全链接层。finetune方法跟各家都类似了，把1000类的softmax换成N+1类的，N是正样本的类别数，1是背景。proposal跟groundtruth的overlap大于0.5的认为是正样本，负样本的overlap在[0.1, 0.5)，不过为何是至少要有0.1的overlap呢？ Semantic Segmentation-Aware CNN featuresMotivation是分割跟检测相关性很强，是啊，我们NIPS的工作也这么想。这个CNN加了个Aware，顿时间觉得好高大上，名字很重要。 网络结构如下： 同样的，网络也分成了两部分： Activation Maps Module: 就是一个FCN，不一样的地方是他们加入了降维：先在VGG上finetune，然后把fc7从4096个channels改为512，然后再finetune一次。label的设置见图，作者为了不引入额外的anntation，他把ground truth bbox里面的像素全部标记为前景，其它的为背景。训练的时候每个类单独训练二元分类器，这样就会产生跟类别数量一样的分割图，这种做法由它设置label的方式决定的，因为物体会存在互相遮挡的情况（不同bounding box有可能存在overlap），一个proposa可能会覆盖多个物体，所以存在某个像素点可以同属于不同的类别，也就是说不能单纯的按照bounding box去设置label（这种label是指0, 1, 2, 3, 4, .., N，N表示类别数），然后只用一个softmax去做分类。举个例子讲，一个像素点属于A类跟属于B类并不是互斥的，这种情况下用2个二元分类器会比用一个有3个label的softmax更靠谱。 Region Adaptation Module: 这个的输入是摘掉了分类层的FCN。然后训练方式就跟每个Multi-region module一样了。但是前面提到Segmentation Maps是用N个二元分类器干的，应该怎么把Activation Maps传给Region Adaptation Module？文章中没提到，我猜想是把N个二元分类器的activation maps拼接在一起然后喂到这个module里面。 最终把这两个features拼在一起就组成了下面的网络： Object Localization大体分为两个步骤：迭代定位(Iterative Localization)跟投票(Bounding box voting)。 Iterative Localization在给proposals打分跟对proposals的bbox进行refine这两步进行迭代。给定proposals，对proposals进行打分，根据分数对proposals进行筛选，对剩下的proposals进行打分并筛选，如此循环，直到收敛。一开始，proposals是由selective search产生，作者说迭代两次就够了。 Bounding box voting是一个普通nms的改进版本，普通的nms是找两个overlap大于一定阈值的proposals，然后把分数低的剔除掉，而作者使用的方法是：假设有proposals的集合为$P = \{p_1, p_2, …, p_n\}$，先找出分数最大的proposal $p_r$，然后找出跟这个proposal的overlap大于一定阈值的所有proposals $P_c$出来(假设$P_c$为空，就选次大的$p_r$，重复上面步骤)，对他们进行位置信息加权平均，权值是他们的分数（要做归一化），然后得到新的proposal $p_new$，把$p_r \cap P_c$从$P$里面删掉，把$p_new$加到$P$里面，继续上面的操作指导无法合并。 示意图如下： Results作者对每个region adaptation module都做了单独的实验，单独的话效果都很一般般。 合起来之后，并且加上作者提到的Localization Scheme之后，效果就飞起来了: PASCAL VOC 2007: PASCAL VOC 2012: pipeline里面的每个component是分离的，每一个component的性能都会影响整体的性能，为什么没人把整个pipeline(至少我目前还没看过)？当然这有一个好处是，每个component可以单独调参数。]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>Detection</tag>
        <tag>CNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记录使用Ubuntu14.04遇到的问题(长期更新)]]></title>
    <url>%2Fposts%2F2015%2F05%2F14%2Fubuntu-14-04-problem%2F</url>
    <content type="text"><![CDATA[不知道为什么我用Linux的系统总是会出现各种各样的问题- -!!!在这里记录一下在使用Ubuntu14.04遇到的问题，以及装各种软件的一些方法。 安装wps官网: http://linux.wps.cn/ 安装前需要安装32位库，不同于以往的版本，ia32-libs已经被抛弃掉，需要安装另外的32位库： 1sudo apt-get install lib32z1 lib32ncurses5 lib32bz2-1.0 libfontconfig1:i386 libXrender1:i386 libsm6:i386 libfreetype6:i386 libglib2.0-0:i386 安装完上面的32库之后才安装wps。启动wps的时候会提示缺失字体，对应的从windows拷过来安装即可。 双显卡问题之前遇到的问题：插入了telsa k40之后，电脑就无法显示了。k40是服务版的显卡，不带视频输出，而BIOS默认是使用外置显卡输出，这就导致了电脑没法显示。解决办法很简单，就是到BIOS把集成显卡（或者是可以用来显示的显卡）作为主要设备，这样就解决了输出的问题了。但是装完NVIDIA的显卡驱动之后，进入ubuntu只能看到桌面背景，除此之外什么也看不到，后来上网找到了解决办法。 本文可以说是上诉解决办法的中文翻译。这种方法是以集成显卡(Intel家的)作为显示，NVIDIA系列的显卡作为CUDA设备为例子。请严格按照下面教程操作，特别是Step 3。 Step 1 (BIOS)到BIOS里面把集显(integrated graphics unit, iGP)设为主要设备。 Step 2 (安装显卡驱动)禁用跟NVIDIA驱动有冲突的驱动把下面的内容写到文件/etc/modprobe.d/blacklist.conf里。写完之后需要重启电脑使得blacklist生效。 123456789101112131415blacklist nouveaublacklist lbm-nouveaublacklist amd76x_edacblacklist vga16fbblacklist rivatvblacklist rivafbblacklist nvidiafbblacklist nvidia-173blacklist nvidia-96blacklist nvidia-currentblacklist nvidia-173-updatesblacklist nvidia-96-updatesalias nvidia nvidia_current_updatesalias nouveau offalias lbm-nouveau off 安装依赖12sudo apt-get install freeglut3 freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev gcc g++ linux-headers-generic linux-sourcesudo ln -s /usr/lib/x86_64-linux-gnu/libglut.so.3 /usr/lib/libglut.so 安装驱动到NVIDIA官网下载驱动。然后进行安装，安装的时候先切换到文本终端(按ctrl+alt+F1，除了F1，也可以是F2 ~ F6任意一个)，并把图形界面给关掉。 123sudo service lightdm stopsudo chmod +x NVIDIA???.runsudo ./NVIDIA???.run 接下来把NVIDIA模块加到Linux内核里面 1sudo modprobe nvidia-uvm 然后使用命令nvidia-smi看驱动是否安装成功。 Step 3 (重装Intel显卡驱动)这步是最最最最关键的。装了显卡驱动之后，如果直接启动图形界面，会发现只能看到桌面背景，这是因为OpenGL库会被NVIDIA的覆盖掉，我们需要重新安装： 1sudo apt-get install --reinstall xserver-xorg-core xserver-xorg-video-intel xserver-xorg-video-glamoregl libgl1-mesa-glx 装完之后就可以打开图形界面了 1sudo service lightdm start Step 4 (安装CUDA)接下来就是安装CUDA了，到官网下载。因为我用的都是CUDA-6.5及之前的版本，这个版本是需要使用gcc 4.6版本的，所以得先安装gcc 4.6。 1sudo apt-get install gcc-4.6 g++-4.6 然后需要让系统把gcc链接到gcc-4.6上。 12345sudo update-alternatives --remove-all gccsudo update-alternatives --config gccsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.6 10sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-4.8 50sudo update-alternatives --config gcc #选择 4.6 然后就直接安装CUDA了。注意在安装的时候会提示你是否安装NVIDIA的显卡驱动，这时候不需要安装NVIDIA显卡驱动，只需要安装CUDA-Toolkit就行。 12sudo chmod +x cuda_6.5???.runsudo ./cuda_???.run 安装完之后就把gcc重新链接到gcc-4.8上。 1sudo update-alternatives --config gcc #选择 4.8 然后把环境变量配好(加到~/.bashrc即可)。 12export PATH=/usr/local/cuda/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH 然后再测试一下安装是否成果，跑去他自带的Sample里面随便找个运行一下就好了。 大功告成，show一下我的显卡: Telsa K40c + GTX TiTan Black，其实还有的，只是我的主板比较弱，只有两个插槽….]]></content>
      <categories>
        <category>折腾</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Nvidia</tag>
        <tag>双显卡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Action Classification In Still Image (CNN篇)]]></title>
    <url>%2Fposts%2F2015%2F05%2F13%2Faction-still-image-cnn%2F</url>
    <content type="text"><![CDATA[赶完ICCV之后，导师还是觉得需要把之前ICME的工作扩展成期刊(当时拿了Best Student Paper的时候已经叫了。。过了快一年了，拖延症啊)，于是开始重新涉猎静态图片下的行为识别，关注在Deep Learning方面的paper，迟点有空会写一下传统的方法。 大概搜了一下，不得不说，Georgia Gkioxari这个美女发了几篇了… Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networkspaper链接 CVPR 2014的paper，文章核心是Transfer Learning，把在大规模的数据集下得到的模型的部分参数迁移到新模型上，一般这方面的工作都是基于在imagenet上训练的模型（AlexNet, VGG），为啥这么做？一般都认为大部分模型前面几层学习的都是一些很普通的特征（如gabor，有人可视化过），后面几层是跟任务相关的，所以通常的做法就是： 先在数据量大的数据集上训练一个模型。 把这个模型的后面几层删掉，换成新的层。 固定没有被换掉的层的参数，新模型在目标数据集上进行finetune。如下图，这篇paper用的AlexNet，把FC8去掉，重新加了两个全链接，在要测试的数据集上进行finetune。在PASCAL 2007/2012上物体分类以及Pascal 2012动作分类上都取得很好效果。 文章另外两个值得学习的部分是：[1] 如何对样本进行分类（设置groundtruth）。会从两个方面考虑(1)正样本的bbox( $B_o$ )跟patch( $P$ )的重合程度，$|P \cap B_o| \ge 0.2|P| \&amp;\&amp; |P \cap B_o| \ge 0.6|B_o|$, (2)一个被当做正样本的patch不能包含两个object。[2] 样本均衡问题。文章的做法是从background patches里面选取10%用来训练，其实我觉得没必要这么做，使用SGD来训练的时候，只需要调整每个mini-batch的正负样本即可，比如一个batch大小是10，可以3个是正样本，7个是负样本，这个方法实质是使得每次更新参数的时候，负样本对权值的影响不会是压倒性的。这个方法我屡试不爽。 在生成样本的时候，每张图片大概会在8个尺度下随机产生500个方形patches，而且这些patches里面，相邻的patches需要有50% overlap，测试也是每张图片用500个patch，然后通过下面的公式进行融合，$C_n$是类别，$M$是patches数量（500），$y(C_n | P_i)$就是每个patch对应的概率输出，$k$是可调参数，$k$越大表示越关心概率大的patch： $$ score(C_n) = \frac{1}{M}y(C_n | P_i)^k $$ 贴一下样本分类的图： Action and Attributes from Wholes and Partspaper链接 我觉得这个paper写得很难懂…. 思路是用CNN做分类，如下图，先通过把人体不同的part检测出来，然后把对每个part提取AlexNet（CNN）的fc7的features，把这些features拼接起来得到concat features，然后把这个concat features作为线性svm的输入进行训练。 然后文章的贡献就是如果检测part，因为还是走的是feature pyramid + sliding windows + classification流程，那么就必须得先训练好单一尺度的part的分类器。文章并不是对人的每个关键点训练一个分类器，而是定义了所谓的high-level part：头/躯干/腿，这相当于把整个人从上到下分成了三部分，下图是例子。 对于每个part，内部还会分不同的类型，比如头部，就可能会被分成左侧脸/右侧脸/正脸，当然这只是我直观上的说法，实际上文章是通过聚类得到的。这跟poselet很像，所以作者也说自己是deep poselet… 同样的，他训练part的分类器跟训练action的分类器类似，也是提取AlexNet某层的feature出来，然后把他扔到线性svm里面训练。在最终测试的时候，part检测是会在给定bbox（可以是groundtruth，可以是检测的结果，例如用R-CNN来检测）的情况下进行检测，对于每个part，会选择在给定bbox下的最大响应作为输出。 R-CNNs for Pose Estimation and Action Detectionpaper链接 这个paper比较简单，一个简单的multi-task paper。网络结构如下： 总共有3个输出，分别是人体检测/姿势估计/动作分类，具体的loss看paper就好了，最终这三个loss是加权起来，通过控制权值可以实现单任务训练以及多任务训练。 跟R-CNN类似，这个网络的输入是object proposal。 Contextual Action Recognition with R*CNNpaper链接 这篇paper是结合了context信息来做，这么看来感觉已经追上了传统的步伐了…套路是：只对整个人建模 –&gt; 对part也建模 –&gt; 对context信息建模，接下来就是对part/context/object同时建模了。 这个paper很简单，可以说是传统的方法换了个特征。尽管如何，还是有点创意的，创意在于作者怎么去对context建模，也就是如何找对行为分类有用的context区域。作者提出了一个R*CNN的方法，他在这个方法里面指定了两个区域，一个叫primary region，这个区域是包括一个人，由ground truth box给出，这个人的动作需要被分类；另外一个叫secondary region，这个区域包含的是对行为分类有用的context信息。secondary regions是由一些region proposal(论文用的是selective search)的方法产生，因为是用region proposal的方法产生，所以可能会存在多个region，那么就存在如何选一个最有效的region出来的问题。 假设$\phi$表示特征，$r$表示primary region，$R(r; I)$表示secondary regions的候选区域，其中$R(r; I) = \{s \in S(I); overlap(s, r) \in [l, u]$，$s$表示secondary region，上面的条件约束了$s$的候选区域，让$s$跟$r$的交并比在一定范围内。$s^*$表示最好的secondary region，那么$$\begin{equation}s^* = arg\max_{s \in R(r; I)}w_s^{\alpha} \cdot \phi(s; I)\end{equation}$$ 最终每个action的score： $$\begin{equation} score(\alpha; I, r) = w_p^{\alpha} \cdot \phi(r; I) + \max_{s \in R(r; I)}w_s^{\alpha} \cdot \phi(s; I) \end{equation}$$ 接着用个softmax转换为概率的形式： $$\begin{equation} P(\alpha | I, r) = \frac{exp(score(\alpha'; I, r)}{\sum{exp(score(\alpha'; I, r)}} \end{equation}$$ 最终上面的公式都可以映射到网络里面，网络结构如下： 作者用的是VGG net，在conv5出来之后就会分两路，红色那路是用来处理primary region，绿色那路是用来处理secondary regions，绿色那里的secondary regions的数量是固定的，每次随机的从$R(r; I)$里面选N个，最终在secondary regions选取一个score最大的跟primary region的score相加，然后经过softmax。实际训练中，fc6/fc7是不动的，finetune的是score这一层的参数。 大概思路就是这样。 然后看一下网络选取secondary regions的情况： 其实可以隐约的发现，secondary regions都是跟action比较相关的，所以说secondary regions还是比较靠谱的，其实这跟Vittorio在12年的PAMI的想法类似的，不过显然，这个模型远不够Vittorio的好。]]></content>
      <categories>
        <category>Academic</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Action</tag>
        <tag>Still Image</tag>
      </tags>
  </entry>
</search>
