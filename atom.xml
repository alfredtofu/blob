<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Marcher</title>
  <icon>https://www.gravatar.com/avatar/b7a8228c05b2543799727a3052722913</icon>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://blog.cvmarcher.cn/"/>
  <updated>2018-04-26T06:29:35.820Z</updated>
  <id>http://blog.cvmarcher.cn/</id>
  
  <author>
    <name>Zhujin Liang</name>
    <email>alfredtofu@gmail.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Face Recognition</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/10/19/Face-Recognition/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/10/19/Face-Recognition/</id>
    <published>2015-10-18T17:08:15.000Z</published>
    <updated>2018-04-26T06:29:35.820Z</updated>
    
    <content type="html"><![CDATA[<p>好久没写博客了。</p><p>最近做了快一个月的人脸识别问题，也算有点经验了。首先放出几个觉得真的有用的paper（至少我能验证有效的）：</p><ol><li><a href="http://arxiv.org/abs/1406.4773" target="_blank" rel="noopener">Deep Learning Face Representation by Joint Identification-Verification</a></li><li><a href="http://arxiv.org/abs/1503.03832" target="_blank" rel="noopener">A Unified Embedding for Face Recognition and Clustering</a></li><li><a href="http://research.microsoft.com/en-us/um/people/jiansun/papers/ECCV12_BayesianFace.pdf" target="_blank" rel="noopener">Bayesian Face Revisited: A Joint Formulation</a></li></ol><a id="more"></a><p>趁着Deep Learning的浪潮，用它来做人脸识别的文章很多，比如DeepID/DeepID2/DeepID2+，DeepFace，FaceNet，Deep Face Recognition(VGG)，大家的效果都做的很不错，DeepFace不好实现，没3D alignment，其他的都还好。前三个是cuhk做的，FaceNet是google做的，Deep Face Recognition是VGG做的，后两个都是训练一个end to end的网络，区别在于VGG还会引入softmax（不同的人为不同的类别，训练一个多元分类器）。</p><h1 id="DeepID2"><a href="#DeepID2" class="headerlink" title="DeepID2"></a>DeepID2</h1><p>DeepID2比较复杂，大概的训练流程如下：</p><ol><li>先训练200个网络（每一个网络都会有两个loss，一个是identification一个是verification），每个网络产生160维特征，每个网络都是同样的结构，但是输入不一样，区别在于：<ul><li>人脸占图片的比例不一样</li><li>会以不同的关键点为图像中心</li><li>图像可以是彩图也可以是灰度图</li><li>左右翻转（这个我觉得应该不会额外训练网络，而是为同一个网络增加训练数据）</li></ul></li><li>然后选择比较有效的25个网络出来，就可以得到25 * 160 = 4000维特征，</li><li>对4k特征做PCA得到180维特征，</li><li>对这180维特征做Joint Bayesian。</li></ol><h1 id="FaceNet-amp-amp-VGG"><a href="#FaceNet-amp-amp-VGG" class="headerlink" title="FaceNet &amp;&amp; VGG"></a>FaceNet &amp;&amp; VGG</h1><p>总的来说，这两个工作很类似，都是用triplet loss，而DeepID2是用Contrastive Loss，而我认为两种形式在一定程度上是等价的（两个pair能转化成triplet）。</p><p>FaceNet跟VGG的区别在于：</p><ol><li>FaceNet有很多很多的数据，</li><li>FaceNet没有用softmax，</li><li>FaceNet加上了hard mining，</li><li>FaceNet的features加上了L2 Norm。</li></ol><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>我做实验的时候，大体的框架是：多网络+L2Norm+triplet loss，多网络的features直接拼起来用两个全连接层再训练一次triplet。由于我是用我之前landmark的工作来对齐人脸的，那个工作只有5个点，所以每个尺度我只产生了5个网络，然后我用了两个尺度，所以总共有10个网络，目前我在LFW上是做到98.7167%。</p><p>下面是简单的记录一下我的实验结论(好像不止这些，心血来潮的时候写这篇博客，记得多少写多少吧)，face-0.5表示以人脸中心作为图像的中心且眼距占输入图像的0.5。</p><h2 id="多网络"><a href="#多网络" class="headerlink" title="多网络"></a>多网络</h2><h3 id="多尺度是否有效"><a href="#多尺度是否有效" class="headerlink" title="多尺度是否有效"></a>多尺度是否有效</h3><p>单个：face-0.5是0.951667，face-0.3是 0.958167，<br>联合是：0.9640<br>联合后加PCA（95% eigenvalues，158 / 320）：0.9652<br>联合后加PCA再加Joint Bayesian：0.9673</p><h3 id="不同的点为中心："><a href="#不同的点为中心：" class="headerlink" title="不同的点为中心："></a>不同的点为中心：</h3><p>单个：face-0.5是0.951667，left-0.3是0.946667，<br>联合是：0.9605<br>联合后加PCA（95% eigenvalues，134 / 320）：0.9622<br>联合后加PCA再加Joint Bayesian：0.9655</p><h2 id="测试的时候加flip"><a href="#测试的时候加flip" class="headerlink" title="测试的时候加flip"></a>测试的时候加flip</h2><p>face-0.5是0.951667，加了flip之后是0.9565<br>flip的加法是：对于A和B两张图片，A1跟A2表示图片A flip前后的照片，对应特征为fa1跟fa2<br>那么最终两张图片的diff为 |fa1-fb1|^2 + |fa1-fb2|^2 + |fa2-fb1|^2 + |fa2-fb2|^2<br>若是直接|fa1-fb1|^2 + |fa2-fb2|^2的准确率是0.956167</p><h2 id="FC代替Joint-Bayesian"><a href="#FC代替Joint-Bayesian" class="headerlink" title="FC代替Joint Bayesian"></a>FC代替Joint Bayesian</h2><p><a href="https://github.com/alfredtofu/Joint_Bayesian" target="_blank" rel="noopener">Joint Bayesian</a>的公式很漂亮，但是毕竟是个生成模型，并不能像判别性模型那样极大的为目标服务，而且用Joint Bayesian也会有一些问题，比如：</p><ol><li>维度一高很难收敛，需要先用PCA降维，但是PCA对维度也很敏感，究竟要降多少维呢？反正每次我用不同维度，结果都不一样。</li><li>Flip的图像后的特征很难统一到模型里面，只能分开算。</li></ol><p>此外我的实验结果也表明用两个全连接层就可以代替Joint Bayesian。</p><h2 id="pair-vs-triplet"><a href="#pair-vs-triplet" class="headerlink" title="pair vs triplet"></a>pair vs triplet</h2><p>triplet收敛很快，快很多，迟点上图。</p><h2 id="分类作用"><a href="#分类作用" class="headerlink" title="分类作用"></a>分类作用</h2><p>没用！不知道是不是我调网络不好，确实真没用。有朋友也尝试过，发现没用。</p><p>此外，假设我有2W个人，像DeepID2这么简单的网络怎么对2w类进行分类？不现实。后面的所有的实验我都不加分类了。FaceNet就很明智的没加了。</p><h2 id="Hard-Mining"><a href="#Hard-Mining" class="headerlink" title="Hard Mining"></a>Hard Mining</h2><p>有用！训练大网络（ZF-net）亲测有效，可以加快收敛。不过我后来也发现你训着训着，剩下不符合margin的基本都是很hard的，所以我后面也不加了…</p><h2 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h2><p>啊，不说什么了，loss稳定降学习率，所有网络都必须有一个点的提升。</p><h2 id="L2-Norm"><a href="#L2-Norm" class="headerlink" title="L2 Norm"></a>L2 Norm</h2><p>好用！加快收敛之余让你训练更加方便，不容易发散。</p><h2 id="数据–数据–数据"><a href="#数据–数据–数据" class="headerlink" title="数据–数据–数据"></a>数据–数据–数据</h2><p>太重要了。。。。。。。。。。。。。。。。。。。谁有更多的数据欢迎共享，MegaFace的数据看起来很难的样子。</p><p><strong>（我感觉是未完待续，不过我感觉自己会懒得来更新了）</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好久没写博客了。&lt;/p&gt;
&lt;p&gt;最近做了快一个月的人脸识别问题，也算有点经验了。首先放出几个觉得真的有用的paper（至少我能验证有效的）：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1406.4773&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Deep Learning Face Representation by Joint Identification-Verification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://arxiv.org/abs/1503.03832&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;A Unified Embedding for Face Recognition and Clustering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://research.microsoft.com/en-us/um/people/jiansun/papers/ECCV12_BayesianFace.pdf&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bayesian Face Revisited: A Joint Formulation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="Face Recognition" scheme="http://blog.cvmarcher.cn/tags/Face-Recognition/"/>
    
      <category term="Face Verification" scheme="http://blog.cvmarcher.cn/tags/Face-Verification/"/>
    
      <category term="Face Identification" scheme="http://blog.cvmarcher.cn/tags/Face-Identification/"/>
    
  </entry>
  
  <entry>
    <title>LSTM for multi-target detection</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/06/29/LSTM-for-multi-target-detection/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/06/29/LSTM-for-multi-target-detection/</id>
    <published>2015-06-29T01:03:14.000Z</published>
    <updated>2018-04-26T06:29:35.820Z</updated>
    
    <content type="html"><![CDATA[<p>论文题目是End-to-end people detection in crowded scenes，<a href="http://arxiv.org/abs/1506.04878" target="_blank" rel="noopener">链接</a></p><p>截止目前，这是我看过的年度最佳论文，可以比肩LeCun做pose的paper，比R-CNN不知道高到哪里去，应该是NIPS2015的paper，你问我支不支持这个paper，我是支持的，我就明确告诉你，我还觉得可以是oral。</p><a id="more"></a><p>这是一个全新的多物体检测的框架（恕我愚昧），也很赶时髦，CNN+LSTM+end to end。以前的物体检测方法大体可以分为两种：sliding window fashion跟基于proposal的分类。像Overfeat就是sliding window fashion一种典型例子，它充分利用了卷积神经网络里面可以共享权值的特性；R-CNN呢，就是基于proposal的分类的例子，流程是这样的：产生proposal –&gt; 对proposal进行分类，分类对的就认为是最终检测的结果。想R-CNN这类方法有没有有点呢？有！直观，而且不用考虑尺度的问题，但是缺点也很明显，很难处理物体overlap比较大以及遮挡比较严重的情况，一般的proposal方法都不能产生这样的proposal。</p><p>回到这篇paper，前面提到他是一个CNN+LSTM的框架，结构图如下所示，CNN用来学习特征，然后LSTM用来产生预测结果。LSTM一个RNN，R字代表recurrent，循环的意思，也正是因为这样，RNN可以产生不固定长度的序列，把这个应用在多物体检测上真是绝配，毕竟多物体检测就是不确定有多少个检测目标。</p><img src="/posts/2015/06/29/LSTM-for-multi-target-detection/net.png"><p>在实际操作中，作者用了GoogLeNet作为特征的提取器，然后LSTM是基于GoogLeNet最顶层的特征(1024维)来进行预测。每一次，LSTM把1024维的特征以及前一个LSTM单元的输出作为下一个LSTM单元输入，每个LSTM单元的输出是一个五元组，用来表示一个检测结果，其中包括这个检测结果的分数以及预测框，当检测结果的分数低于某个阈值的时候，LSTM就停止forward。</p><p>然后Loss的定义就比较中规中矩，如下所示。</p><img src="/posts/2015/06/29/LSTM-for-multi-target-detection/loss.png"><p>作者是首先将预测结果跟groundtruth做一次匹配，每个groundtruth只能跟一个预测结果匹配上，然后只对于匹配上的预测结果会惩罚它的预测框，所有的预测分数都会做一次惩罚。</p><p>最后还是感叹一下，这个思路真的很好，把多物体检测跟LSTM完美结合在一起，嵌入到CNN里面。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;论文题目是End-to-end people detection in crowded scenes，&lt;a href=&quot;http://arxiv.org/abs/1506.04878&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;截止目前，这是我看过的年度最佳论文，可以比肩LeCun做pose的paper，比R-CNN不知道高到哪里去，应该是NIPS2015的paper，你问我支不支持这个paper，我是支持的，我就明确告诉你，我还觉得可以是oral。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="Detection" scheme="http://blog.cvmarcher.cn/tags/Detection/"/>
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
      <category term="LSTM" scheme="http://blog.cvmarcher.cn/tags/LSTM/"/>
    
  </entry>
  
  <entry>
    <title>导数/偏导数/方向导数/梯度</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/06/27/gradient-descent/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/06/27/gradient-descent/</id>
    <published>2015-06-27T09:41:40.000Z</published>
    <updated>2018-04-26T06:29:35.868Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Gradient_descent" target="_blank" rel="noopener">梯度下降法</a>是求解神经网络的方法中最流行的一个，思想很简单，就是函数沿着梯度的方向下降的最快。通常来讲，我们在求解机器学习问题的时候，都会定义一个目标函数，然后基于这个目标函数又定义出损失函数，通过最小化损失函数来使得目标函数达到最优。那么在最小化损失函数的时候就可以用上梯度下降了。</p><a id="more"></a><p>思想简单，实现也很简单。在这篇文章里面，我主要是想讲讲<strong>梯度</strong>这个东西，因为我经常会被这个概念搞糊涂掉。梯度大一高数(记忆中高中也是讲过…)有教。什么是梯度？首先，它是一个向量，那向量肯定会有方向嘛，梯度的方向呢是使得方向导数达到最大值的方向，它的模就是方向导数的最大值。那什么是方向导数？理解这个东西需要知道导数这个概念，下面我打算从数学定义来说明导数/偏导数/方向导数/梯度这四个东西。</p><h1 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h1><p>这个我想没有人会不知道。</p><blockquote><strong>定义</strong>：<br>当函数<span>$y=f(x)$</span><!-- Has MathJax -->的自变量在一点<span>$x_0$</span><!-- Has MathJax -->上产生一个增量<span>$\Delta x$</span><!-- Has MathJax -->时，函数输出值的增量与自变量增量<span>$\Delta x$</span><!-- Has MathJax -->的比值在<span>$\Delta x$</span><!-- Has MathJax -->趋于<span>$0$</span><!-- Has MathJax -->时的极限如果存在，即为<span>$f(x)$</span><!-- Has MathJax -->在<span>$x_0$</span><!-- Has MathJax -->处的导数，记作<span>$f'(x_0)$</span><!-- Has MathJax -->、<span>$\frac{\mathrm{d}f}{\mathrm{d}x}(x_0)$</span><!-- Has MathJax -->或<span>$\left.\frac{\mathrm{d}f}{\mathrm{d}x}\right|_{x=x_0}$</span><!-- Has MathJax --><br></blockquote><p>我们从小到大对导数的认识就是认为它是函数曲线在相应点的切线的斜率。</p><h1 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h1><p>慢慢的，我们当然不能仅限于一元函数的情况，那么多元函数求导是一个怎么概念？在多元函数情况下，通常我们计算的是<strong>偏导数</strong>，那么什么是偏导数？简单来讲，就是函数只对某个变量求导得到的导数就是函数关于这个变量的偏导数。比方说，对于二元函数<span>$z = f(x, y)$</span><!-- Has MathJax -->，当我们把$y$固定住，然后对$x$求导，那么得到的导数称为$f(x, y)$对$x$的偏导数，记为<span>$\frac{\mathrm{d}z}{\mathrm{d}x}$</span><!-- Has MathJax -->。同样的，固定$x$，对$y$求导得到的便是对$y$的偏导数，记为<span>$\frac{\mathrm{d}z}{\mathrm{d}y}$</span><!-- Has MathJax -->。</p><h1 id="方向导数"><a href="#方向导数" class="headerlink" title="方向导数"></a>方向导数</h1><p>那么实际上，偏导数<span>$\frac{\mathrm{d}z}{\mathrm{d}x}$</span><!-- Has MathJax -->跟<span>$\frac{\mathrm{d}z}{\mathrm{d}y}$</span><!-- Has MathJax -->是函数沿着相应的坐标轴方向的变化率(因为每次我们都固定住其它的变量)，那么如果需要同时考虑其他方向的变化率，该怎么办？这就引申出方向导数的概念了。</p><blockquote><strong>定义</strong>：<br>设$z = f(x, y)$在一点<span>$P_0(x_0, y_0)$</span><!-- Has MathJax -->的一个邻域内有定义，又设<span>$\overrightarrow{l}$</span><!-- Has MathJax -->是给定的一个方向，其方向余弦为<span>$(cos \alpha, cos \beta)$</span><!-- Has MathJax -->，若极限<br><span>$$\begin{equation}\lim_{t \to 0}{\frac{f(x_0 + tcos \alpha, y_0 + tcos \beta) - f(x_0, y_0)}{t}}\end{equation}$$</span><!-- Has MathJax -->存在，则称此极限值为函数$z = f(x, y)$在<span>$P_0$</span><!-- Has MathJax -->点沿方向<span>$\overrightarrow{l}$</span><!-- Has MathJax -->的方向导数，记为<span>$\left.\frac{\mathrm{d}z}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0}$</span><!-- Has MathJax --><br></blockquote><p>实际上，<span>$\left.\frac{\mathrm{d}z}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0}$</span><!-- Has MathJax -->是等于<span>$\left.\frac{\mathrm{d}z}{\mathrm{d}x}\right|_{x_0, y_0} cos \alpha + \left.\frac{\mathrm{d}z}{\mathrm{d}y}\right|_{x_0, y_0} cos \beta$</span><!-- Has MathJax --></p><p>其中$t$为点<span>$P_0(x_0, y_0)$</span><!-- Has MathJax -->到点<span>$P_t(x_0 + tcos \alpha, y_0 + tcos \beta)$</span><!-- Has MathJax -->的距离。</p><p>很容易推广到多元的情况，比如三元函数$u = f(x, y, z)$在点<span>$P_0(x_0, y_0, z_0)$</span><!-- Has MathJax -->沿方向<span>$\overrightarrow{l}$</span><!-- Has MathJax -->的方向导数如下面是式子，<span>$\overrightarrow{l}$</span><!-- Has MathJax -->方向的方向余弦为<span>$(cos \alpha, cos \beta, cos \gamma)$</span><!-- Has MathJax --></p><span>$$\begin{equation}\left.\frac{\mathrm{d}u}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0, z_0} = \left.\frac{\mathrm{d}u}{\mathrm{d}x}\right|_{x_0, y_0, z_0}cos \alpha + \left.\frac{\mathrm{d}u}{\mathrm{d}y}\right|_{x_0, y_0, z_0}cos \beta + \left.\frac{\mathrm{d}u}{\mathrm{d}y}\right|_{x_0, y_0, z_0}cos \gamma\end{equation}$$</span><!-- Has MathJax --><h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>因为引入了方向导数，那么自然而然就会想，在同一点上的所有方向导数中是否有最大值？显然是有的，而且这个方向导数就是<strong>梯度</strong>了！</p><p>假设令<span>$\overrightarrow{g} = (\left.\frac{\mathrm{d}z}{\mathrm{d}x}\right|_{x_0, y_0}, \left.\frac{\mathrm{d}z}{\mathrm{d}y}\right|_{x_0, y_0})$</span><!-- Has MathJax -->，那么就有</p><span>$$\begin{aligned}\left.\frac{\mathrm{d}z}{\mathrm{d} \overrightarrow{l} }\right|_{x_0, y_0}   & = \overrightarrow{g} \cdot \overrightarrow{l} \\  & = | \overrightarrow{g} | | \overrightarrow{l} | cos \langle \overrightarrow{g}, \overrightarrow{l} \rangle \\  & = | \overrightarrow{g} | cos \langle \overrightarrow{g}, \overrightarrow{l} \rangle\end{aligned}$$</span><!-- Has MathJax --><p>显然，当<span>$\langle \overrightarrow{g}, \overrightarrow{l} \rangle = 0$</span><!-- Has MathJax -->的时候，也就是<span>$\overrightarrow{g} \textrm{跟} \overrightarrow{l}$</span><!-- Has MathJax -->同向的时候，函数关于<span>$\overrightarrow{l}$</span><!-- Has MathJax -->的方向导数最大，且最大值为<span>$| \overrightarrow{g} |$</span><!-- Has MathJax -->。</p><p>这里，<span>$\overrightarrow{g} = (\left.\frac{\mathrm{d}z}{\mathrm{d}x}\right|_{x_0, y_0}, \left.\frac{\mathrm{d}z}{\mathrm{d}y}\right|_{x_0, y_0})$</span><!-- Has MathJax -->就成为函数在点<span>$P_0(x_0, y_0)$</span><!-- Has MathJax -->的梯度，记为<span>$\textrm{grad}\ z |_{x_0, y_0}$</span><!-- Has MathJax -->。</p><p>梯度下降法也是从这里来，当我们在求解的时候没办法直接得到最优解，只能不断逼近最优解，而逼近的时候又想尽可能的快，那么只能沿着函数变化最剧烈的方法(也就是梯度方向)的反方向走。这里顺便提一下梯度上升法，其实是跟梯度下降法一样，只是它是用于求解函数的最大值，在逼近最优解的时候，沿着梯度方向走。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Gradient_descent&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;梯度下降法&lt;/a&gt;是求解神经网络的方法中最流行的一个，思想很简单，就是函数沿着梯度的方向下降的最快。通常来讲，我们在求解机器学习问题的时候，都会定义一个目标函数，然后基于这个目标函数又定义出损失函数，通过最小化损失函数来使得目标函数达到最优。那么在最小化损失函数的时候就可以用上梯度下降了。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
  </entry>
  
  <entry>
    <title>Notes for Faster R-CNN and YOLO</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/06/10/Notes-for-Faster-R-CNN-and-YOLO/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/06/10/Notes-for-Faster-R-CNN-and-YOLO/</id>
    <published>2015-06-10T11:39:24.000Z</published>
    <updated>2018-04-26T06:29:35.824Z</updated>
    
    <content type="html"><![CDATA[<p>最近NIPS 2015的投稿截止了，各种投到NIPS的文章也外泄了，看到了两篇关于Detection的文章，说实话，看到这些论文的时候一点兴奋都没有。</p><p>两篇paper都是出自rbg大神的: <a href="http://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a>以及<a href="http://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a>。然后两篇都是讲提速的，共同点都是把Selective Search(一些额外的proposal方法)去掉，用网络本身产生，做成一个end-to-end的框架。</p><a id="more"></a><h1 id="Faster-R-CNN"><a href="#Faster-R-CNN" class="headerlink" title="Faster R-CNN"></a>Faster R-CNN</h1><p>说白了是一个能够联合优化的Cascade结构。</p><p>核心思想是引入了一个Region Proposal Networks(RPNs)，这个网络产生的proposals作为Fast R-CNN的输入。在特征提取上RPNs跟Fast R-CNN共享前面的所有卷积层，流程可简化如下：</p><img src="/posts/2015/06/10/Notes-for-Faster-R-CNN-and-YOLO/faster_R-CNN.jpg"><p>其中RPNs的结构如下:</p><img src="/posts/2015/06/10/Notes-for-Faster-R-CNN-and-YOLO/faster_R-CNN_rpn.png"><p>实际上操作的时候，会用RPNs在最后一层的特征图(Feature Map)上做划窗(Sliding window)，窗口的大小是$n \textrm{x} n$，然后每个窗口经过RPNs后会产生$k$个proposals，为什么不是一个而是使用$k$个？看文章的意思是为了得到平移/尺度不变形，不过在我看来是没必要的，这样给人感觉太过hand-crafted了，直接回归一个bbox也就可以了。作者好像没有对$k$做对比实验。</p><p>然后具体是怎么联合优化的？实际上，一开始就直接把整个网络进行联合优化是不合理的，因为proposal都不准，训练Fast R-CNN的分类网络意义何在….所以作者就提出了自己的训练方法，一开始，RPNs跟Fast R-CNN的卷积层是不共享的：</p><ol><li>用ImageNet-pre-trained(AlexNet或VGG)来初始化RPN，然后对RPN进行fine-tune。</li><li>用ImageNet-pre-trained(AlexNet或VGG)来初始化Fast R-CNN，然后对Fast R-CNN进行fine-tune，其中这时候Fast R-CNN的proposals输入是由训练好的RPN产生的。</li><li>用Fast R-CNN来初始化RPN，固定卷积层，fine-tune全连接层。</li><li>固定卷积层，同时fine-tune RPN跟Fast R-CNN。</li></ol><p>方法不太优美，但是又好像只能这么做。</p><h1 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h1><p>这个很暴力…</p><p>只是大概讲一下思路，首先看一下YOLO的模型：</p><img src="/posts/2015/06/10/Notes-for-Faster-R-CNN-and-YOLO/yolo_net.png"><p>我想你应该没想错，就是这么暴力，对于一张图片，不管多大，都是先resize成448x448，然后扔进网络里面，网络就输出7x7=49个bbox，同时会为每个bbox输出它属于某种物体的概率。</p><p>那么具体这个7x7跟原图有什么关系以及groundtruth应该怎么设？作者是把原图划分成7x7=49个格子，然后如果原图上的某个物体的中点落在某个格子$b_{xy}$上，那么这个格子就负责预测这个物体出来，比如狗的bbox的中点落在(1, 5)这个格子，那么这个格子对应的groundtruth就是狗的bbox，以及狗的概率为1。</p><p>这有一个很严重的缺陷是，它没办法处理物体挨在一起且中点都落在同一个格子上的情况。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近NIPS 2015的投稿截止了，各种投到NIPS的文章也外泄了，看到了两篇关于Detection的文章，说实话，看到这些论文的时候一点兴奋都没有。&lt;/p&gt;
&lt;p&gt;两篇paper都是出自rbg大神的: &lt;a href=&quot;http://arxiv.org/abs/1506.01497&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks&lt;/a&gt;以及&lt;a href=&quot;http://arxiv.org/abs/1506.02640&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;You Only Look Once: Unified, Real-Time Object Detection&lt;/a&gt;。然后两篇都是讲提速的，共同点都是把Selective Search(一些额外的proposal方法)去掉，用网络本身产生，做成一个end-to-end的框架。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="Detection" scheme="http://blog.cvmarcher.cn/tags/Detection/"/>
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>从SVN到Git，代码的合并</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/06/04/svn-to-git/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/06/04/svn-to-git/</id>
    <published>2015-06-04T04:59:14.000Z</published>
    <updated>2018-04-26T06:29:35.896Z</updated>
    
    <content type="html"><![CDATA[<p>之前一直用的是旧版本的caffe，然后用svn管理，每次官方改了bug，都是手动加进去，很蛋疼….</p><p>最近一怒之下转到git上了，那么问题来了，原来的版本我改了很多，怎么合并？以前没接触git，最多就是从github上clone项目下来，对merge啊，branch啊什么的完全没概念。折腾了一晚上终于搞掂了，期间大部分时间是花在手工修改代码上，下面讲讲我的经历。</p><a id="more"></a><h1 id="将我的代码跟最新的caffe代码合并"><a href="#将我的代码跟最新的caffe代码合并" class="headerlink" title="将我的代码跟最新的caffe代码合并"></a>将我的代码跟最新的caffe代码合并</h1><p>首先，先clone一份最新的caffe代码：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">https</span>://github.com/BVLC/caffe.git</span><br></pre></td></tr></table></figure><p>然后新建一个空白分支，用来放我的代码</p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout <span class="comment">--orphan zhujin</span></span><br><span class="line">rm -r *</span><br></pre></td></tr></table></figure><p>然后把我的代码全拷贝进来，并做一次commit</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">add</span><span class="bash"> *</span></span><br><span class="line"><span class="bash">git commit -m <span class="string">'add my codes'</span></span></span><br></pre></td></tr></table></figure><p>接着切换到master上，并进行合并：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout <span class="literal">master</span></span><br><span class="line">git merge <span class="keyword">master</span> <span class="title">zhujin</span></span><br></pre></td></tr></table></figure><p>这时候一般是提示会有冲突，没办法，使用下面命令开始手动清理：</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">git mergetool</span></span><br></pre></td></tr></table></figure><p>清理完毕后，再看一下使用<code>git status</code>再看一下有没有冲突，如果没有，commit一次就完成了合并了。</p><p>通常弄完之后我会删除刚才新建的分支:</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">git</span> <span class="keyword">branch </span>-d zhujin</span><br></pre></td></tr></table></figure><h1 id="创建本地仓库"><a href="#创建本地仓库" class="headerlink" title="创建本地仓库"></a>创建本地仓库</h1><p>想必大家都不想这么快把自己的代码公开吧，毕竟有时候需要保密，这时候自己搭建一个git服务器是最安全而且方便的。那么先把现有仓库导出为裸仓库，</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">--bare</span> caffe caffe.git</span><br></pre></td></tr></table></figure><p>这样会生成一个叫caffe.git的文件夹，然后把它拷贝到你的服务器上，或者说本地的某个目录，这个文件夹就是仓库了，下次直接向这里进行clone/pull/push。我把这个文件夹放到了我的网盘上，直接clone就好：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone <span class="regexp">/home/</span>liangzhujin<span class="regexp">/wangpan/</span>kuaipan<span class="regexp">/repos/gi</span>trepos<span class="regexp">/caffe.git</span></span><br></pre></td></tr></table></figure><h1 id="合并两个不同repository的分支"><a href="#合并两个不同repository的分支" class="headerlink" title="合并两个不同repository的分支"></a>合并两个不同repository的分支</h1><p>现在遇到的问题是，因为我搭建了服务器，不能直接使用git pull从官方的repository更新了，这样很坑爹啊，如果官方修复了一些bug，那我还得手动的一个一个改回来。</p><p>然而，git还是很强大的，能够合并不同地址的两个分支。</p><p>首先，我本地服务器的git地址是/home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe.git，先进行clone</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone <span class="regexp">/home/</span>liangzhujin<span class="regexp">/wangpan/</span>kuaipan<span class="regexp">/repos/gi</span>trepos<span class="regexp">/caffe.git</span></span><br></pre></td></tr></table></figure><p>这时候可以使用命令<code>git remote -v</code>查看你当前跟踪的repository有哪些，在我这里显示的只有我本地服务器的分支</p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">origin  <span class="meta-keyword">/home/</span>liangzhujin<span class="meta-keyword">/wangpan/</span>kuaipan<span class="meta-keyword">/repos/</span>gitrepos/caffe.git (fetch)</span><br><span class="line">origin  <span class="meta-keyword">/home/</span>liangzhujin<span class="meta-keyword">/wangpan/</span>kuaipan<span class="meta-keyword">/repos/</span>gitrepos/caffe.git (push)</span><br></pre></td></tr></table></figure><p>此外，还可以用命令<code>git branch -av</code>看看目前的branch，显示：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">* <span class="keyword">master</span>                <span class="title">8853960</span> fix bug, all test case pass except hdf5</span><br><span class="line">  remotes/origin/HEAD   -&gt; origin/<span class="literal">master</span></span><br><span class="line">  remotes/origin/<span class="keyword">master</span> <span class="title">8853960</span> fix bug, all test case pass except hdf5</span><br></pre></td></tr></table></figure><p>然后把caffe官方的repository也加进来，注意的是，<code>BVLC</code>是自己取的名字，用来区分不同的repository。</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote <span class="built_in">add</span> BVLC http<span class="variable">s:</span>//github.<span class="keyword">com</span>/BVLC/caffe.git</span><br></pre></td></tr></table></figure><p>这时候使用命令<code>git remote -v</code>就发现你跟踪的repository多了。我这里显示的是：</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">BVLC    <span class="string">https:</span><span class="comment">//github.com/BVLC/caffe.git (fetch)</span></span><br><span class="line">BVLC    <span class="string">https:</span><span class="comment">//github.com/BVLC/caffe.git (push)</span></span><br><span class="line">origin  <span class="regexp">/home/</span>liangzhujin<span class="regexp">/wangpan/</span>kuaipan<span class="regexp">/repos/</span>gitrepos/caffe.git (fetch)</span><br><span class="line">origin  <span class="regexp">/home/</span>liangzhujin<span class="regexp">/wangpan/</span>kuaipan<span class="regexp">/repos/</span>gitrepos/caffe.git (push)</span><br></pre></td></tr></table></figure><p>这时候需要使用命令<code>git fetch BVLC</code>从官网把最新的branch下载下来，如果有下面提示表示成功下载了。</p><figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">From https:<span class="comment">//github.com/BVLC/caffe</span></span><br><span class="line"> * [<span class="function"><span class="title">new</span> branch]      dev        -&gt;</span> BVLC/dev</span><br><span class="line"> * [<span class="function"><span class="title">new</span> branch]      device-abstraction -&gt;</span> BVLC/device-abstraction</span><br><span class="line"> * [<span class="function"><span class="title">new</span> branch]      gh-pages   -&gt;</span> BVLC/gh-pages</span><br><span class="line"> * [<span class="function"><span class="title">new</span> branch]      master     -&gt;</span> BVLC/master</span><br><span class="line"> * [<span class="function"><span class="title">new</span> branch]      parallel   -&gt;</span> BVLC/parallel</span><br><span class="line"> * [<span class="function"><span class="title">new</span> branch]      tutorial   -&gt;</span> BVLC/tutorial</span><br></pre></td></tr></table></figure><p>这时候再看一下分支，<code>git branch -av</code>：<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">* <span class="keyword">master</span>                          <span class="title">8853960</span> fix bug, all test case pass except hdf5</span><br><span class="line">  remotes/BVLC/dev                <span class="number">9</span>b0662f Merge pull request <span class="comment">#1461 from ixartz/fix_convert_mnist_siamese_data</span></span><br><span class="line">  remotes/BVLC/device-abstraction d52d669 Fix post-rebase errors, including: -device-abstracted <span class="keyword">version</span> of MVN -new memset/memcpy wrappers (set_void <span class="keyword">and</span> copy_void) -fixing MKL switching logic</span><br><span class="line">  remotes/BVLC/gh-pages           b7c55d7 ae4a5b1 Merge pull request <span class="comment">#2505 from ronghanghu/matcaffe3</span></span><br><span class="line">  remotes/BVLC/<span class="keyword">master</span>             <span class="title">72d7089</span> [bug] fix double instantiation of GPU methods <span class="keyword">in</span> LogLayer</span><br><span class="line">  remotes/BVLC/parallel           aa3b877 Distributed training</span><br><span class="line">  remotes/BVLC/tutorial           e99d108 add tutorial link</span><br><span class="line">  remotes/origin/HEAD             -&gt; origin/<span class="literal">master</span></span><br><span class="line">  remotes/origin/<span class="keyword">master</span>           <span class="title">8853960</span> fix bug, all test case pass except hdf5</span><br><span class="line">`</span><br></pre></td></tr></table></figure></p><p>然后切换到本地的分支，并进行合并：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git checkout <span class="literal">master</span></span><br><span class="line">git merge BVLC/<span class="literal">master</span></span><br></pre></td></tr></table></figure><p>当然，合并前可以看看你的分支跟将要合并的分支的区别：</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff <span class="keyword">master</span> <span class="title">BVLC</span>/<span class="literal">master</span></span><br></pre></td></tr></table></figure><p>merge完之后，还要更新到本地服务器:<code>git push</code>，OK，All is done.</p><h1 id="更新代码"><a href="#更新代码" class="headerlink" title="更新代码"></a>更新代码</h1><p>好了，如果遇到caffe更新了代码怎么办？很简单，切换到BVLC/master分支，更新一下，然后切回master进行合并即可。</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git checkout BVLC/<span class="literal">master</span></span><br><span class="line">git pull</span><br><span class="line">git checkout <span class="literal">master</span></span><br><span class="line">git merge</span><br></pre></td></tr></table></figure><hr><p><strong>Update</strong></p><hr><h1 id="搭建git服务器"><a href="#搭建git服务器" class="headerlink" title="搭建git服务器"></a>搭建git服务器</h1><p>搭建git服务器也很简单，git是用ssh来访问的，用的是22端口，只要支持ssh访问就可以直接当做git服务器来用。那么得先安装OpenSSH服务:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> openssh-<span class="keyword">server</span></span><br></pre></td></tr></table></figure><p>这时候其实也不需要做什么了，git服务器已经可以用了…clone的地址变成:</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone liangzhujin<span class="variable">@127</span>.<span class="number">0</span>.<span class="number">0</span>.<span class="number">1</span><span class="symbol">:/home/liangzhujin/wangpan/kuaipan/repos/gitrepos/caffe</span>.git</span><br></pre></td></tr></table></figure><p>上面，liangzhujin是用户名(这里的用户名是ssh访问时候用的用户名)，127.0.0.1是你git服务器的地址。</p><p>对了，如果你的仓库是放在家目录下的，那地址可以更简单，这里假设<code>caffe.git</code>就在<code>/home/liangzhujin</code>下:</p><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">liangzhujin</span>@<span class="number">127.0</span>.<span class="number">0.1</span>:caffe.git</span><br></pre></td></tr></table></figure><p>假设你需要用<strong>别的账号</strong>来使用git服务，而且这些账号是不能登陆shell的，比如我想新建一个用户git，这个账号只能用来使用git的服务器，而不能ssh登陆，那么方法也很简单:</p><h2 id="新建用户"><a href="#新建用户" class="headerlink" title="新建用户"></a>新建用户</h2><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">useradd git</span></span><br></pre></td></tr></table></figure><h2 id="禁用shell登陆"><a href="#禁用shell登陆" class="headerlink" title="禁用shell登陆"></a>禁用shell登陆</h2><p>修改<code>/etc/passwd</code>文件:</p><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">git:</span><span class="symbol">x:</span><span class="number">118</span><span class="symbol">:</span><span class="number">126</span><span class="symbol">:</span><span class="symbol">:/home/git</span><span class="symbol">:/bin/bash</span></span><br><span class="line">改成</span><br><span class="line"><span class="symbol">git:</span><span class="symbol">x:</span><span class="number">118</span><span class="symbol">:</span><span class="number">126</span><span class="symbol">:</span><span class="symbol">:/home/git</span><span class="symbol">:/usr/bin/git-shell</span></span><br></pre></td></tr></table></figure><h2 id="修改仓库的拥有者"><a href="#修改仓库的拥有者" class="headerlink" title="修改仓库的拥有者"></a>修改仓库的拥有者</h2><p>假设仓库是<code>caffe.git</code>，需要把它的拥有者跟组设为<code>git</code>:<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">chown</span> <span class="selector-tag">-R</span> <span class="selector-tag">git</span><span class="selector-pseudo">:git</span> <span class="selector-tag">caffe</span><span class="selector-class">.git</span></span><br></pre></td></tr></table></figure></p><p>然后把<code>caffe.git</code>放到<code>/home/git/</code>下。</p><h2 id="进行clone"><a href="#进行clone" class="headerlink" title="进行clone"></a>进行clone</h2><p>这时候，就可以进行clone了:<br><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="keyword">clone</span> <span class="title">git</span>@<span class="number">127.0</span>.<span class="number">0.1</span>:caffe.git</span><br></pre></td></tr></table></figure></p><p>另外，如果想建立严格的权限控制，就要用到<a href="https://github.com/sitaramc/gitolite" target="_blank" rel="noopener">Gitolite</a></p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="https://git-scm.com/book/zh/v1/%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84-Git-%E5%9C%A8%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E9%83%A8%E7%BD%B2-Git" target="_blank" rel="noopener">服务器上的 Git - 在服务器上部署 Git</a></li><li><a href="https://git-scm.com/book/zh/v1/Git-%E5%88%86%E6%94%AF-%E5%88%86%E6%94%AF%E7%9A%84%E6%96%B0%E5%BB%BA%E4%B8%8E%E5%90%88%E5%B9%B6" target="_blank" rel="noopener">Git 分支 - 分支的新建与合并</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前一直用的是旧版本的caffe，然后用svn管理，每次官方改了bug，都是手动加进去，很蛋疼….&lt;/p&gt;
&lt;p&gt;最近一怒之下转到git上了，那么问题来了，原来的版本我改了很多，怎么合并？以前没接触git，最多就是从github上clone项目下来，对merge啊，branch啊什么的完全没概念。折腾了一晚上终于搞掂了，期间大部分时间是花在手工修改代码上，下面讲讲我的经历。&lt;/p&gt;
    
    </summary>
    
      <category term="折腾" scheme="http://blog.cvmarcher.cn/categories/%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="Git" scheme="http://blog.cvmarcher.cn/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>The Role of Features, Algorithms and Data in Visual Recognition</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/</id>
    <published>2015-06-02T01:07:11.000Z</published>
    <updated>2018-04-26T06:29:35.824Z</updated>
    
    <content type="html"><![CDATA[<p>最近被华农的老师邀请去给她的研究生介绍一下Deep Learning，一时间没想到什么好的角度去讲。</p><p>看了不少大牛们关于Deep Learning的introduction，结合最近Deep Learning三大神牛Nature上的文章，还是发现了不少的东西，有些问题甚至自己都没想过的。</p><p>慢慢的就形成了做PPT的思路了，从<strong>特征工程</strong>引出<strong>特征学习</strong>，然后再从<strong>特征学习</strong>引出<strong>深度学习</strong>，然后再讲几个经典的深度模型，重点讲讲CNN。</p><p><strong>update</strong>: slides做好了，提前放出来。下载链接: </p><a id="more"></a><h1 id="Pipeline-of-Pattern-Recognition"><a href="#Pipeline-of-Pattern-Recognition" class="headerlink" title="Pipeline of Pattern Recognition"></a>Pipeline of Pattern Recognition</h1><p>模式识别里面所有的任务都可以归结为下面一个流程：</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/model-of-pattern-recognition.png"><p>正是因为这个流程，计算机视觉的发展都是围绕三个大的方面来进行：</p><ol><li><strong>数据</strong>。如何清洗数据？如何对数据做扰动，增加训练样本？等等等等。</li><li><strong>特征表达</strong>。如何找到一种更有效的特征表达方式？抑或说怎么得到更加适合特定任务的特征，从而增加模型的准确性。</li><li><strong>学习算法</strong>。怎么设计出一种高效准确的学习算法？</li></ol><p>毫无疑问的是，数据、特征表达以及学习算法都是非常重要的，不过哪个会比较重要？这么说或许不合理，应该说，在哪个方面下功夫对最终的结果影响最大？</p><h1 id="Role-of-three-factors"><a href="#Role-of-three-factors" class="headerlink" title="Role of three factors"></a>Role of three factors</h1><p>CVPR 2010年的一篇文章给出了线索，它做了大量的实验去比较跟验证哪个更重要，甚至还用做了人跟机器的性能比较，在这里我就不关心人的效果到底怎么样了。</p><h2 id="The-Role-of-Learning-Algorithm"><a href="#The-Role-of-Learning-Algorithm" class="headerlink" title="The Role of Learning Algorithm"></a>The Role of Learning Algorithm</h2><p>首先比较了不同<strong>学习算法</strong>对结果的影响，实验方法是固定特征跟训练数据，使用不同的学习算法进行求解。下图就是实验结果，横轴是特征，纵轴是准确率，不同的子图表示用不同的数据集，不同颜色的曲线表示不同的学习算法。可以发现不同学习算法他们的效果会有区别，但是变化不会太大。</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/compare_learning_algorithm.png"><h2 id="The-Role-of-Data"><a href="#The-Role-of-Data" class="headerlink" title="The Role of Data"></a>The Role of Data</h2><p>然后就是探究<strong>训练数据</strong>的不同对结果的影响，这时候就需要固定学习算法跟特征，使用不同的训练数据进行训练，下图是他们的实验结果，横轴是训练数据的数量，纵轴就是准确率，不同的子图表示不同的数据集以及不同的特征，不同颜色的曲线表示不同的学习算法，发现数据量增大的时候，效果提升还是挺明显的，然而到了一定量之后，数据增加，效果并不会提升了或者说提升非常慢。</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/compare_data.png"><h2 id="The-Role-of-Features"><a href="#The-Role-of-Features" class="headerlink" title="The Role of Features"></a>The Role of Features</h2><p>最后就是探究特征的作用，这时候需要固定学习算法跟特征。下图是实验结果，横轴表示不同的学习算法，纵轴表示准确率，不同的子图表示不同数据集以及特征维度，不同颜色的曲线表示不同的特征，可以看出来，特征的选择对结果影响非常大。</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/compare_features.png"><p>通过这组对比实验，可以得出的结论是，<strong>特征很重要</strong>，特征一旦没选好，不管用什么算法，用多少数据量、用什么再强大的算法也没什么用，比如上面提到的红色的这条曲线，不管用多少数据，用什么学习算法，它的效果总是最烂的。相反，如果特征选得好，用再简单的算法也能达到相当不错的效果。</p><h1 id="Feature-Engineering"><a href="#Feature-Engineering" class="headerlink" title="Feature Engineering"></a>Feature Engineering</h1><p>那么这样就会引发我们的思考，特征是影响效果的最重要因素，然而在目前的主流系统里面，用的都是各种各样手工定义的特征，这些手工定义的特征有不少的缺陷：</p><ul><li><p>很依赖于人的领域知识，那么比如如果一个学计算机视觉的人想要做车辆的检测，那么需要花很多时间去了解车辆，区分不同车辆之间的关系。</p></li><li><p>特征的设计跟学习算法是分开的，这样的一个坏处是，如果效果好，并不知道是特征好还是学习算法不好，难以分析。</p></li><li>如果设计的特征有很多参数，那么就很难进行调参。</li><li>如果出现了一个新的任务，那么设计有效的特征会非常缓慢。</li></ul><p>比如下面这个图，我们想要学习出一张图片里面的深度信息，所谓的深度信息就是，物体距离我的距离，左边是一个正确的距离表示，不同颜色表示距离的远近，然而，这种任务的特征需要怎么设计？这非常困难，可以说是无从入手。</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/depth_estimation.png"><p>这时候自然而然就会想，能不能找到更有效的特征，能不能直接从数据里面学习出物体的特征，使得特征更适合我们需要解决的问题？</p><h1 id="Feature-Learning"><a href="#Feature-Learning" class="headerlink" title="Feature Learning"></a>Feature Learning</h1><p>这当然是可以的，而且也一直被研究，叫特征学习。只不过在以前的时代，数据量不多，计算不够快，导致了这样的研究很难进行下去。</p><p>特征学习是一个比较久的概念了，它指的是建立分类器或者一些预测器的同时，学习出数据的一种转换，使得基于这种转换能够更容易的提取出有效的信息，为最终的任务服务，特征学习的方法的好处在于：</p><ul><li>能够把分类器跟特征的表达一起学习，并做联合优化。</li><li>在这种方法底下，我们可以为特征表示设计出大量的参数并进行学习，这样可以极大的提高了深度模型的表达能力。通常来讲，参数越多，模型的表达能力会越强。</li><li>因为是直接从数据里面学习特征，那么会更有效的挖掘数据本身的特性。</li><li>对于新的应用领域，能够很快的得到一个比较好的特征表达方式。</li></ul><p>那么有什么方法是可以用来学习特征的？</p><h1 id="Deep-Learning"><a href="#Deep-Learning" class="headerlink" title="Deep Learning"></a>Deep Learning</h1><p>时下最火的Deep Learning就是一种非常强大的特征学习方法，他能够层次性的学习特征，高层的特征是用底层的特征表示的。这跟我们人的认知是很吻合的，像我们初中学的几何，我们就知道点动成线，线动成面，一样的道理，我们对一个物体的认识，通常都会把他进行拆解，比如人脸这个东西，通常我们理解是对他进行拆解成眼耳口鼻，然后不同的部分又是由不同的形状组合而成。右边这个图呢，是生物学的人对人脑的研究，发现脑对事物的认知是分层的，从简单到复杂。深度学习正是仿照这个脑认知来设计的，左边这个图是一个很好的说明，模型的输入直接是图像，然后一开始模型是学习出比较简单的特征，比如边缘，接着学习出更加复杂的特征，比如物体的部件，越往高层，特征越复杂，越接近我们想要了解的东西。</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/feature_learning_example.jpg"><p>Deep Learning比较经典的模型有三个：DBN、Auto-Encoder以及CNN，这里主要讲一下CNN，对其他有兴趣的自行深入了解。</p><h2 id="Convolutional-Neural-Networks-CNN"><a href="#Convolutional-Neural-Networks-CNN" class="headerlink" title="Convolutional Neural Networks (CNN)"></a>Convolutional Neural Networks (CNN)</h2><p>卷积神经网络，跟很多深度模型都不太一样，像DBN、Auto-Encoder等都是把数据拉成一个一维的向量处理，而CNN是把数据以多维的数组来处理，比如图像通常会表示为3个二维数组，每个二维数组表示一个颜色通道，RGB。</p><p>神经元在卷积层会被组织成特征图的形式，每一张特征图是一个二维数组，比如下图的第2~4个子图，都是特征图，在特征图里面，每个点只会跟前一层的某个小区域有关。这种连接关系会决定了参数的个数，比如假设特征图上的一个点跟前面的一个4x4的区域有连接，那么这时候就只会有16个参数，对于一个特征图上点来讲，他们都共享这些参数。这些参数我们通常就叫做滤波器，右下角就是把滤波器可视化后的结果。而pooling操作了是指将特征图相邻的神经元进行合并。</p><img src="/posts/2015/06/02/The-Role-of-Features-Algorithms-and-Data-in-Visual-Recognition/cnn.png"><p>卷积神经网络里面有4个非常有技巧的设计，1) 局部连接， 2) 共享权重， 3) pooling，以及 4) 多层卷积的使用，它们都利用了自然信号的特性。我逐一讲解一下为什么这些东西是有效的。</p><ul><li>对于局部连接，对于二维的数据，特别是图像，局部区域的值是高度相关的，而跟远离自己的区域的相关性很弱，所以不需要对太大的区域进行建模，这也是我们经常提到的图像局部区域相关性。在生物视觉的研究里面，当人注视前方的时候，只会关注前面的一小个区域，而不关注与周围的区域，按照这个理解，建模的时候不需要对太大的区域进行建模，需要对一小个区域建模就好了，这就是所谓的局部连接。</li><li>而对于共享权重，前面讲过，同一个特征图上的神经元是共享相同的参数的，理由是图像或者其他信号的局部统计信息是跟位置无关的，出现在哪里都是可以的，这有点抽象，举个例子，比如人脸的眼睛，它是可以出现在图像上的任意位置的，如果一个特征图是表示眼睛出现的概率，那么这个特征图上的神经元就自然而然的会共享同一组参数。</li><li>接下来是pooling，这个比较好理解，主要是用了降维，相邻的神经元表达的东西是很相似的，这时候需要对这些冗余的信息进行减少，加速计算速度 。</li><li>多个卷积层的使用，这是一个分层的概念了，多层主要是为了学习到更高层的语义特征。在卷积神经网络里面，底层的特征都是一些比较简单的特征，比如边缘，再往高层就是由边缘组成的部件，再往更高层就是由部件组成的物体了。</li></ul><p>因为卷积神经网络有上面这些特性，导致了它的参数比普通的深度模型少很多，从而使得卷积神经网络比以前更加容易训练。所以最近很多工作都是基于卷积神经网络来做的。</p><p>以上。</p><h1 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h1><ol><li><a href="http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html" target="_blank" rel="noopener">Deep Learning, Nature, 2015</a></li><li><a href="http://research.microsoft.com/en-us/um/people/larryz/FADParikhZitnick_CVPR2010.pdf" target="_blank" rel="noopener">The Role of Features, Algorithms and Data in Visual Recognition</a></li><li><a href="https://sites.google.com/site/deeplearningcvpr2014/" target="_blank" rel="noopener">Deep Learning Tutorial, CVPR 2014</a></li><li><a href="https://piazza.com/cuhk.edu.hk/spring2015/eleg5040/home" target="_blank" rel="noopener">Introduction to Deep Learning, Xiaogang Wang</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近被华农的老师邀请去给她的研究生介绍一下Deep Learning，一时间没想到什么好的角度去讲。&lt;/p&gt;
&lt;p&gt;看了不少大牛们关于Deep Learning的introduction，结合最近Deep Learning三大神牛Nature上的文章，还是发现了不少的东西，有些问题甚至自己都没想过的。&lt;/p&gt;
&lt;p&gt;慢慢的就形成了做PPT的思路了，从&lt;strong&gt;特征工程&lt;/strong&gt;引出&lt;strong&gt;特征学习&lt;/strong&gt;，然后再从&lt;strong&gt;特征学习&lt;/strong&gt;引出&lt;strong&gt;深度学习&lt;/strong&gt;，然后再讲几个经典的深度模型，重点讲讲CNN。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;update&lt;/strong&gt;: slides做好了，提前放出来。下载链接: &lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
      <category term="Feature Engineering" scheme="http://blog.cvmarcher.cn/tags/Feature-Engineering/"/>
    
      <category term="Feature Learning" scheme="http://blog.cvmarcher.cn/tags/Feature-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Action Classification In Still Image (CNN篇)-2</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/29/action-still-image-cnn2/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/29/action-still-image-cnn2/</id>
    <published>2015-05-28T16:13:07.000Z</published>
    <updated>2018-04-26T07:14:29.230Z</updated>
    
    <content type="html"><![CDATA[<p>之前也写过几篇用CNN做静态图片的行为分类的<a href="/posts/2015/05/13/action-still-image-cnn">论文笔记</a>。最近又看了一些，继续写笔记。</p><a id="more"></a><h1 id="Recognize-Complex-Events-from-Static-Images-by-Fusing-Deep-Channels"><a href="#Recognize-Complex-Events-from-Static-Images-by-Fusing-Deep-Channels" class="headerlink" title="Recognize Complex Events from Static Images by Fusing Deep Channels"></a>Recognize Complex Events from Static Images by Fusing Deep Channels</h1><p><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Xiong_Recognize_Complex_Events_2015_CVPR_paper.pdf" target="_blank" rel="noopener">paper链接</a></p><p>最近连续被Xiaoou刺激…Action的ideas都被做了，Video的<a href="http://arxiv.org/abs/1505.04868" target="_blank" rel="noopener">idea</a>被做，Still Image的也被做，大体的思路都一样，就是细节不一样，不过现在看来只能再想别的模型了。</p><p>这个paper说的是识别Event，这跟Action的分类是非常类似的，Action一般需要识别的是单个人，而Event关注的是一群人或者整张图片表示的事件，尽管如此，方法上还是非常像的。</p><p>文章核心想法是多信息融合。融合分两步，物体跟人的信息先进行一次融合(这能够对物体跟人的interaction进行建模)，融合后的特征再跟全局(全图)的信息融合，这种做法是很有道理的，物体跟人的交互往往是非常重要的线索(这么讲吧，如果能够很精确的检测物体的存在，这个Event是什么基本能够知道了，不过对于一些跟fine-grained的Event讲，交互/位置信息就很重要了，抱狗跟亲狗这两个Event，单单把狗检测出来还不够，还需要知道人头跟狗头的位置关系才能进一步区分)，然后全局的信息也是很重要的，我往往把这全局信息理解为场景信息，比如区分室内室外等等的。</p><p>用的依旧是CNN，既然是CNN的东西了，那么直接上网络结构就能懂了80%，网络结构如下：</p><img src="/posts/2015/05/29/action-still-image-cnn2/xiaoou_fuse_deep_channels_net.png"><p>接下来就根据不同的信息来讲吧，重点还是人跟物体这块，也就是网络结构图的下半部分。</p><h2 id="全局信息"><a href="#全局信息" class="headerlink" title="全局信息"></a>全局信息</h2><p>这里直接上了AlexNet，看样子是取fc6出来做特征(4096维)，仍旧用ImageNet来pre-train，这没什么好讲了。</p><h2 id="物体跟人"><a href="#物体跟人" class="headerlink" title="物体跟人"></a>物体跟人</h2><p>因为数据集并没有给出人跟物体的位置，需要先对物体跟人的进行检测，检测出来之后，作者并没有直接把他们的坐标并起来，而是定义了一些特征，然后再让网络去学。</p><h3 id="Feature-for-Human"><a href="#Feature-for-Human" class="headerlink" title="Feature for Human"></a>Feature for Human</h3><p>对于人的检测，作者并没有简单的用一个人的检测器，因为这很困难，同时如果人检测不到，那就谈不上interaction了，所以为了加强人的检测，作者还使用了人脸的检测，两者进行互补。(其实换成人头肩检测更好啊…)</p><p>那么如何构建人的特征？简单来讲，作者构建了binary maps，bboxes框住的地方的值为1，其他为0，下面是一个例子。</p><img src="/posts/2015/05/29/action-still-image-cnn2/xiaoou_fuse_deep_channels_confusion.png"><p>看了上面的图你也会发现，两个不同的Event，它们的特征几乎一样啊，这怎么区分？为了解决这个问题，作者就引入了多尺度的map，意思是，不同的大小的bboxes，会激活不同尺度的map，这个激活是指把bboxes框住的地方的值设为1。这里的不同大小的比较，是要在归一化的空间里面比较，归一化的方法是把原图resize成18x18，然后再算这时候bbox的面积。作者设置了两个阈值，这就划分出了3个空间，一个空间对应一个尺度，所以最终会得出3张map。人脸跟人都要做这样的操作，最终就会得出6张maps。例子例子：</p><img src="/posts/2015/05/29/action-still-image-cnn2/xiaoou_fuse_deep_channels_multi_scale.png"><h3 id="Feature-for-Object"><a href="#Feature-for-Object" class="headerlink" title="Feature for Object"></a>Feature for Object</h3><p>同样的方式构建特征。不过只有一个尺度。</p><p>作者进行了统计，得出了最常出现的30个物体，别问我为什么是30个….这时候就有30个maps了。</p><h3 id="Fusion-of-Human-and-Object"><a href="#Fusion-of-Human-and-Object" class="headerlink" title="Fusion of Human and Object"></a>Fusion of Human and Object</h3><p>把6张maps跟30张maps拼起来就完成fusion了！接着就是架个CNN学特征了，看网络结构图即可，最终得到4096维的特征。</p><h2 id="Fusion-of-All"><a href="#Fusion-of-All" class="headerlink" title="Fusion of All"></a>Fusion of All</h2><p>这里是对全局特征跟物体和人的特征进行融合，作者说是一种semantic fusion，将两者的4096维特征进行融合得到新的4096维特征，不过好像没说清楚怎么融合，加？乘？</p><h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>因为没有现有的数据集，作者自己搞了个数据集，所以他实现了几个方法跟自己对比：</p><img src="/posts/2015/05/29/action-still-image-cnn2/xiaoou_fuse_deep_channels_results.png"><p>效果还是一如既往的不高，这个领域可提升空间真的很大。能看出CNN的威力了，作者的方法在Top 1对比其他CNN的方法提升不少，不过在Top 5区别就不大了。</p><p>最后说一句，最近都流行搞数据集了….我们实验室也是各种搞，这样有两个好处是，你可以很容易把别人PK掉，而且由于有数据集发布，你的引用也会增加不少。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前也写过几篇用CNN做静态图片的行为分类的&lt;a href=&quot;/posts/2015/05/13/action-still-image-cnn&quot;&gt;论文笔记&lt;/a&gt;。最近又看了一些，继续写笔记。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
      <category term="Action" scheme="http://blog.cvmarcher.cn/tags/Action/"/>
    
      <category term="Still Image" scheme="http://blog.cvmarcher.cn/tags/Still-Image/"/>
    
  </entry>
  
  <entry>
    <title>在Centos 6.5搭建VPN服务器</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/27/set-up-vpn-in-centos-6-5/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/27/set-up-vpn-in-centos-6-5/</id>
    <published>2015-05-27T14:53:47.000Z</published>
    <updated>2018-04-26T06:29:35.892Z</updated>
    
    <content type="html"><![CDATA[<p>为什么要搭，都懂的…在电脑端主要是用ssh作为socks5代理(Windows用Bitvise SSH Client，Linux用<code>ssh -N -D 7070 用户名@IP地址</code>)，将http的报文封装在ssh的数据包里，在客户端和服务器之间传输，这个跟Chrome的插件SwitchyOmega(<a href="/posts/2015/05/27/set-up-vpn-in-centos-6-5/OmegaOptions.bak" title="我的备份文件">我的备份文件</a>)搭配起来超级方便。不过手机上没办法用(iPhone得越狱，我没越狱)，只能再搭个VPN了。</p><a id="more"></a><h1 id="首先检查需要搭建VPN的服务器有没有启用PPP以及TUN"><a href="#首先检查需要搭建VPN的服务器有没有启用PPP以及TUN" class="headerlink" title="首先检查需要搭建VPN的服务器有没有启用PPP以及TUN"></a>首先检查需要搭建VPN的服务器有没有启用PPP以及TUN</h1><p>使用下面命令:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /dev/ppp</span><br><span class="line">cat /dev/net/tun2</span><br></pre></td></tr></table></figure></p><p>如果显示结果是:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat: /dev/ppp: <span class="literal">No</span> such device <span class="keyword">or</span> address</span><br><span class="line">cat: /dev/net/tun: File descriptor <span class="keyword">in</span> bad state</span><br></pre></td></tr></table></figure></p><p>那么表示已经启用了。如果没启用，让你的服务提供商给你开吧。</p><h1 id="安装必须要的软件"><a href="#安装必须要的软件" class="headerlink" title="安装必须要的软件"></a>安装必须要的软件</h1><p>需要用到<code>ppp</code>, <code>iptables</code><br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install<span class="built_in"> ppp </span>iptables</span><br></pre></td></tr></table></figure></p><p>添加pptpd的源并安装:<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -i http:<span class="regexp">//</span>poptop.sourceforge.net<span class="regexp">/yum/</span>stable<span class="regexp">/rhel6/</span>pptp-release-current.noarch.rpm</span><br><span class="line">yum install pptpd</span><br></pre></td></tr></table></figure></p><h1 id="配置pptp"><a href="#配置pptp" class="headerlink" title="配置pptp"></a>配置pptp</h1><p>修改各种配置文件:</p><h2 id="etc-pptpd-conf"><a href="#etc-pptpd-conf" class="headerlink" title="/etc/pptpd.conf"></a>/etc/pptpd.conf</h2><p>添加下面两行:<br><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localip <span class="number">192.168</span><span class="number">.0</span><span class="number">.1</span></span><br><span class="line">remoteip <span class="number">192.168</span><span class="number">.0</span><span class="number">.10</span><span class="number">-111</span></span><br></pre></td></tr></table></figure></p><p><code>192.168.0.10-111</code>表示用户被分配的ip地址的范围。</p><h2 id="etc-ppp-options-pptpd"><a href="#etc-ppp-options-pptpd" class="headerlink" title="/etc/ppp/options.pptpd"></a>/etc/ppp/options.pptpd</h2><p>添加下面两行，表示使用Google的DNS服务器:<br><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">ms-dns</span> 8<span class="selector-class">.8</span><span class="selector-class">.8</span><span class="selector-class">.8</span></span><br><span class="line"><span class="selector-tag">ms-dns</span> 8<span class="selector-class">.8</span><span class="selector-class">.4</span><span class="selector-class">.4</span></span><br></pre></td></tr></table></figure></p><h2 id="etc-ppp-chap-secrets"><a href="#etc-ppp-chap-secrets" class="headerlink" title="/etc/ppp/chap-secrets"></a>/etc/ppp/chap-secrets</h2><p>这个文件是用来添加vpn账号的，每一行表示一个用户，格式如下:<br><figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">username</span> pptpd passwd *</span><br></pre></td></tr></table></figure></p><ul><li><code>username</code>表示用户名</li><li><code>passwd</code>表示账号</li><li><code>*</code>表示任何ip，如果只想一个账号只能一个人登陆，就设置一个固定ip，参考上面的ip地址范围。我试过忘了填，导致vpn一直连不上，弄了很久才发现是这个问题…</li></ul><h2 id="etc-sysctl-conf"><a href="#etc-sysctl-conf" class="headerlink" title="/etc/sysctl.conf"></a>/etc/sysctl.conf</h2><p>修改内核设置，使得支持转发，将<code>net.ipv4.ip_forward=0</code>改为<code>net.ipv4.ip_forward=1</code>，要取消注释。修改后运行下面命令使得生效:<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/sbin/</span>sysctl -p</span><br></pre></td></tr></table></figure></p><h1 id="添加转发规则"><a href="#添加转发规则" class="headerlink" title="添加转发规则"></a>添加转发规则</h1><p>上面配置完了，重启<code>pptpd</code>服务后，应该是能登陆，但是没法上网，这时候需要添加iptables转发规则:<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables -t<span class="built_in"> nat </span>-A POSTROUTING -s 192.168.0.0/24 -j SNAT --to-source server-ip</span><br></pre></td></tr></table></figure></p><ul><li><code>192.168.0.0/24</code>: 这个是需要包括你上面分配的ip地址范围</li><li><code>vps-ip</code>: 这里填的是你的服务器ip</li></ul><p>保存iptables转发规则:<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/etc/i</span>nit.d<span class="regexp">/iptables save</span></span><br></pre></td></tr></table></figure></p><p>然后重启<code>iptables</code>以及<code>pptpd</code>:<br><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta-keyword">/etc/</span>init.d/iptables restart</span><br><span class="line"><span class="meta-keyword">/etc/</span>init.d/pptpd restart</span><br></pre></td></tr></table></figure></p><p>这时候就可以登陆并上网了。</p><h1 id="设置开机启动"><a href="#设置开机启动" class="headerlink" title="设置开机启动"></a>设置开机启动</h1><p>把<code>iptables</code>以及<code>pptpd</code>都设置为开机启动:<br><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chkconfig iptables <span class="keyword">on</span></span><br><span class="line">chkconfig pptpd <span class="keyword">on</span></span><br></pre></td></tr></table></figure></p><h1 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h1><p>Google或者百度一下<code>vpn pptp设置</code>即可。</p><p>OK，搞掂。</p><hr><p><strong>我是很帅气的分割线</strong></p><hr><p>下面写个别的问题，就是解决用ssh代理无法登陆google的问题。会提示如下错误：<br><figure class="highlight vbnet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">We<span class="comment">'re sorry...</span></span><br><span class="line">... but your computer <span class="keyword">or</span> network may be sending automated queries. <span class="keyword">To</span> protect our users, we can<span class="comment">'t process your request right now.</span></span><br><span class="line">See Google Help <span class="keyword">for</span> more information.</span><br></pre></td></tr></table></figure></p><p>百度搜了一下，是Google把linode的ipv6给屏蔽了，只需要把ipv6禁用了就可以了，方法如下:<br><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo <span class="string">'1'</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/</span>ipv6<span class="regexp">/conf/</span>lo/disable_ipv6</span><br><span class="line">echo <span class="string">'1'</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/</span>ipv6<span class="regexp">/conf/</span>all/disable_ipv6</span><br><span class="line">echo <span class="string">'1'</span> &gt; <span class="regexp">/proc/</span>sys<span class="regexp">/net/</span>ipv6<span class="regexp">/conf/</span><span class="keyword">default</span>/disable_ipv6</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;为什么要搭，都懂的…在电脑端主要是用ssh作为socks5代理(Windows用Bitvise SSH Client，Linux用&lt;code&gt;ssh -N -D 7070 用户名@IP地址&lt;/code&gt;)，将http的报文封装在ssh的数据包里，在客户端和服务器之间传输，这个跟Chrome的插件SwitchyOmega(&lt;a href=&quot;/posts/2015/05/27/set-up-vpn-in-centos-6-5/OmegaOptions.bak&quot; title=&quot;我的备份文件&quot;&gt;我的备份文件&lt;/a&gt;)搭配起来超级方便。不过手机上没办法用(iPhone得越狱，我没越狱)，只能再搭个VPN了。&lt;/p&gt;
    
    </summary>
    
      <category term="折腾" scheme="http://blog.cvmarcher.cn/categories/%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="Centos" scheme="http://blog.cvmarcher.cn/tags/Centos/"/>
    
      <category term="VPN" scheme="http://blog.cvmarcher.cn/tags/VPN/"/>
    
      <category term="PPTP" scheme="http://blog.cvmarcher.cn/tags/PPTP/"/>
    
  </entry>
  
  <entry>
    <title>在Centos 6.5配置FTP服务器</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/27/set-up-ftp-in-centos-6-5/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/27/set-up-ftp-in-centos-6-5/</id>
    <published>2015-05-27T14:06:44.000Z</published>
    <updated>2018-04-26T06:29:35.892Z</updated>
    
    <content type="html"><![CDATA[<p>因为要帮我哥写个软件，要用到存储服务，就自己搭了个FTP。在Centos 6.5下。主要是参考<a href="http://blog.sina.com.cn/s/blog_459ced7a0101ou76.html" target="_blank" rel="noopener">这个博客</a>，后面根据自己的需求改了点东西。</p><a id="more"></a><h1 id="安装vsftpd，ftp的服务器"><a href="#安装vsftpd，ftp的服务器" class="headerlink" title="安装vsftpd，ftp的服务器"></a>安装vsftpd，ftp的服务器</h1><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum <span class="keyword">install</span> vsftpd</span><br></pre></td></tr></table></figure><h1 id="设置开机启动"><a href="#设置开机启动" class="headerlink" title="设置开机启动"></a>设置开机启动</h1><figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chkconfig vsftpd <span class="keyword">on</span></span><br></pre></td></tr></table></figure><h1 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h1><figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service vsftpd <span class="literal">start</span></span><br></pre></td></tr></table></figure><h1 id="配置FTP用户组-用户以及相应权限"><a href="#配置FTP用户组-用户以及相应权限" class="headerlink" title="配置FTP用户组/用户以及相应权限"></a>配置FTP用户组/用户以及相应权限</h1><h2 id="添加用户组"><a href="#添加用户组" class="headerlink" title="添加用户组"></a>添加用户组</h2><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">groupadd ftp</span></span><br></pre></td></tr></table></figure><h2 id="添加用户"><a href="#添加用户" class="headerlink" title="添加用户"></a>添加用户</h2><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -g ftp -M -d <span class="regexp">/srv/</span>ftp<span class="regexp">/zhujin -s /</span>sbin<span class="regexp">/nologin zhujin</span></span><br></pre></td></tr></table></figure><ul><li><code>-g</code>接的是用户组</li><li><code>-M</code>表示不设置它的主目录，假设如果没有<code>-M</code>，则在<code>/home</code>下会有跟用户名(<code>zhujin</code>)一样的目录。</li><li><code>-d</code>后面接的是用<code>zhujin</code>登陆FTP的时候，它的初始目录。</li><li><code>-s</code> 后面接<code>/sbin/nologin</code>表示用户不需要登录系统，因为我们只需要用来登陆FTP</li><li><code>zhujin</code>表示用户名了</li></ul><p>设置刚才添加的用户的密码<br><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">passwd zhujin</span></span><br></pre></td></tr></table></figure></p><h2 id="更改FTP目录的权限"><a href="#更改FTP目录的权限" class="headerlink" title="更改FTP目录的权限"></a>更改FTP目录的权限</h2><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chown -R <span class="string">zhujin:</span>ftp <span class="regexp">/srv/</span>ftp/zhujin</span><br></pre></td></tr></table></figure><p>这时候重启vsftpd<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/etc/i</span>nit.d<span class="regexp">/vsftpd restart</span></span><br></pre></td></tr></table></figure></p><h2 id="把用户限制在固定的目录"><a href="#把用户限制在固定的目录" class="headerlink" title="把用户限制在固定的目录"></a>把用户限制在固定的目录</h2><p>如果这时候登陆会发现刚才新建的用户可以访问并读取所有的目录的数据，这并不是我们想要的，需要把他们限定在某个目录下。修改配置文件<br>vsftpd.conf，目录一般在<code>/etc/vsftpd/vsftpd.conf</code>，添加下面两行：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">chroot_list_enable</span>=<span class="literal">YES</span></span><br><span class="line"><span class="attr">chroot_list_file</span>=/etc/vsftpd/chroot_list</span><br></pre></td></tr></table></figure></p><p>然后在文件<code>/etc/vsftpd/chroot_list</code>里面填入你想要限制的用户，比如我就填入了<code>zhujin</code>，这时候重启vsftp，然后重新登陆就可以了。</p><h1 id="设置匿名用户以及它的根目录"><a href="#设置匿名用户以及它的根目录" class="headerlink" title="设置匿名用户以及它的根目录"></a>设置匿名用户以及它的根目录</h1><h2 id="允许匿名用户登陆"><a href="#允许匿名用户登陆" class="headerlink" title="允许匿名用户登陆"></a>允许匿名用户登陆</h2><p>需要修改配置文件vsftpd.conf，添加下面内容：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">anonymous_enable</span>=<span class="literal">YES</span></span><br></pre></td></tr></table></figure></p><h1 id="设置匿名用户的根目录"><a href="#设置匿名用户的根目录" class="headerlink" title="设置匿名用户的根目录"></a>设置匿名用户的根目录</h1><p>需要修改配置文件vsftpd.conf，添加下面内容：<br><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">anon_root</span>=/srv/ftp/anon</span><br></pre></td></tr></table></figure></p><p>完成后重启一下vsftpd<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="regexp">/etc/i</span>nit.d<span class="regexp">/vsftpd restart</span></span><br></pre></td></tr></table></figure></p><p>以上</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;因为要帮我哥写个软件，要用到存储服务，就自己搭了个FTP。在Centos 6.5下。主要是参考&lt;a href=&quot;http://blog.sina.com.cn/s/blog_459ced7a0101ou76.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这个博客&lt;/a&gt;，后面根据自己的需求改了点东西。&lt;/p&gt;
    
    </summary>
    
      <category term="折腾" scheme="http://blog.cvmarcher.cn/categories/%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="Centos" scheme="http://blog.cvmarcher.cn/tags/Centos/"/>
    
      <category term="vsftpd" scheme="http://blog.cvmarcher.cn/tags/vsftpd/"/>
    
      <category term="FTP" scheme="http://blog.cvmarcher.cn/tags/FTP/"/>
    
  </entry>
  
  <entry>
    <title>使用Caffe时候遇到的问题(长期更新)</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/21/caffe-problem/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/21/caffe-problem/</id>
    <published>2015-05-21T14:51:37.000Z</published>
    <updated>2018-04-26T06:29:35.868Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/BVLC/caffe/" target="_blank" rel="noopener">Caffe</a>是一个深度学习的框架，本文主要是记录使用Caffe遇到的一些奇葩问题，不涉及对代码的理解。</p><a id="more"></a><h1 id="Caffe的环境配置"><a href="#Caffe的环境配置" class="headerlink" title="Caffe的环境配置"></a>Caffe的环境配置</h1><p>为了方便，我把我在Ubuntu 14.04下编译的<a href="http://caffe.berkeleyvision.org/installation.html" target="_blank" rel="noopener">依赖库</a>上传到<a href="http://pan.baidu.com/s/1pJHqWR5" target="_blank" rel="noopener">百度云</a>，下载并修改<code>Makefile.config</code>即可编译Caffe，免去各种复杂的环境配置。在Ubuntu 12.04跟14.04都可用。</p><h1 id="Anaconda的libm-so跟系统库的冲突"><a href="#Anaconda的libm-so跟系统库的冲突" class="headerlink" title="Anaconda的libm.so跟系统库的冲突"></a>Anaconda的libm.so跟系统库的冲突</h1><p>提示的错误如下:</p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libx<span class="number">264</span>.so.<span class="number">142</span>: undefined reference <span class="keyword">to</span> `__exp_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libx<span class="number">264</span>.so.<span class="number">142</span>: undefined reference <span class="keyword">to</span> `__log<span class="number">10</span>_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libxvidcore.so.<span class="number">4</span>: undefined reference <span class="keyword">to</span> `__logf_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libvorbis.so.<span class="number">0</span>: undefined reference <span class="keyword">to</span> `__acosf_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libx<span class="number">264</span>.so.<span class="number">142</span>: undefined reference <span class="keyword">to</span> `__pow_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libx<span class="number">264</span>.so.<span class="number">142</span>: undefined reference <span class="keyword">to</span> `__log<span class="number">2</span>_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libxvidcore.so.<span class="number">4</span>: undefined reference <span class="keyword">to</span> `__log<span class="number">10</span>f_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libxvidcore.so.<span class="number">4</span>: undefined reference <span class="keyword">to</span> `__log_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libx<span class="number">264</span>.so.<span class="number">142</span>: undefined reference <span class="keyword">to</span> `__powf_finite<span class="title">@GLIBC_2.15</span>'</span><br><span class="line">//usr/lib/<span class="keyword">x</span><span class="number">86</span>_<span class="number">64</span>-linux-gnu/libx<span class="number">264</span>.so.<span class="number">142</span>: undefined reference <span class="keyword">to</span> `__log<span class="number">2</span>f_finite<span class="title">@GLIBC_2.15</span>'</span><br></pre></td></tr></table></figure><p>解决办法是把Anaconda的libm.so删掉或者改名，让Caffe编译的时候链接的是系统自带的。Anaconda的libm.so的路径是<code>$HOME/anaconda/lib/libm.so</code></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://github.com/BVLC/caffe/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Caffe&lt;/a&gt;是一个深度学习的框架，本文主要是记录使用Caffe遇到的一些奇葩问题，不涉及对代码的理解。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Caffe" scheme="http://blog.cvmarcher.cn/tags/Caffe/"/>
    
  </entry>
  
  <entry>
    <title>Concepts and Tricks In CNN(长期更新)</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/17/cnn-trick/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/17/cnn-trick/</id>
    <published>2015-05-17T13:48:41.000Z</published>
    <updated>2018-04-26T06:29:35.868Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章主要讲一下Convolutional Neural Network(CNN)里面的一些概念以及技巧。</p><a id="more"></a><h1 id="Receptive-Field-感受野"><a href="#Receptive-Field-感受野" class="headerlink" title="Receptive Field (感受野)"></a>Receptive Field (感受野)</h1><p>这是一个非常重要的概念，receptive field往往是描述两个feature maps A/B上神经元的关系，假设从A经过若干个操作得到B，这时候B上的一个区域<span>$\textrm{area}_b$</span><!-- Has MathJax -->只会跟a上的一个区域相关<span>$\textrm{area}_a$</span><!-- Has MathJax -->，这时候<span>$\textrm{area}_a$</span><!-- Has MathJax -->成为<span>$\textrm{area}_b$</span><!-- Has MathJax -->的感受野。用图片来表示：</p><img src="/posts/2015/05/17/cnn-trick/receptive_field.jpg"><p>在上图里面，map 3里1x1的区域对应map 2的receptive field是那个红色的7x7的区域，而map 2里7x7的区域对应于map 1的receptive field是蓝色的11x11的区域，所以map 3里1x1的区域对应map 1的receptive field是蓝色的11x11的区域。</p><p>那么很容易得出来，receptive field的计算公式如下：</p><ul><li>对于Convolution/Pooling layer:</li></ul><span>$$\begin{equation}r_i = s_i \cdot (r_{i + 1} - 1) + k_i\end{equation}$$</span><!-- Has MathJax --><p>其中$r_i$表示第$i$层layer的输入的某个区域，$s_i$表示第$i$层layer的步长，$k_i$表示kernel size，注意，不需要考虑padding size。</p><ul><li>对于Neuron layer(ReLU/Sigmoid/…)</li></ul><span>$$\begin{equation}r_i = r_{i + 1}\end{equation}$$</span><!-- Has MathJax --><h1 id="Coordinate-Mapping"><a href="#Coordinate-Mapping" class="headerlink" title="Coordinate Mapping"></a>Coordinate Mapping</h1><p>通常，我们需要知道网络里面任意两个feature map之间的坐标映射关系，如下图，我们想得到map 3上的点$p_3$映射回map 2所在的位置$p_2$。</p><img src="/posts/2015/05/17/cnn-trick/coordinate_map.jpg"><p>计算公式如下：</p><ul><li>对于Convolution/Pooling layer:</li></ul><span>$$\begin{equation}p_i = s_i \cdot p_{i + 1} + (\frac{k_i - 1}{2} - \textrm{padding}_i)\end{equation}$$</span><!-- Has MathJax --><p>其中$p_i$表示第$i$层layer的输入的某个点，$s_i$表示第$i$层layer的步长，$k_i$表示kernel size，<span>$\textrm{padding}_i$</span><!-- Has MathJax --></p><ul><li>对于Neuron layer(ReLU/Sigmoid/…)</li></ul><span>$$\begin{equation}p_i = p_{i + 1}\end{equation}$$</span><!-- Has MathJax --><p>上面是计算任意一个layer输入输出的坐标映射关系，如果是计算任意feature map之间的关系，只需要用简单的组合就可以得到，下图是一个简单的例子：</p><img src="/posts/2015/05/17/cnn-trick/coordinate_map_example.jpg"><h1 id="Convolutionalize-卷积化"><a href="#Convolutionalize-卷积化" class="headerlink" title="Convolutionalize (卷积化)"></a>Convolutionalize (卷积化)</h1><p>最近掀起了<a href="http://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">FCN(全卷积网络)</a>风，这种网络里面不包括全连接层(fully connected layer)。</p><h2 id="卷积层跟全连接层的区别"><a href="#卷积层跟全连接层的区别" class="headerlink" title="卷积层跟全连接层的区别"></a>卷积层跟全连接层的区别</h2><p><strong>卷积层</strong>的操作跟传统的<strong>滑窗</strong>(sliding windows)很相似，把kernel作用于输入的不同的区域然后产生对应的特征图，由于这样的性质，给定一个卷积层，它并不要求输入是固定大小的，它可能根据输入大小的不同而产生大小不一样的特征图。</p><img src="/posts/2015/05/17/cnn-trick/sliding-windows.png"><p><strong>全连接层</strong>的操作是把输入拉成一个一维的向量，然后对这一维的向量进行点乘，这就要求输入大小是固定的。</p><p>那么如果使用一个包含fc层的模型(如AlexNet)就必须使用固定大小的输入，其实有时候这是非常不方便以及不合理的，比如下图，如果我要把红框的塔输入网络，就必须得对它进行变成，假设是放到AlexNet里面，因为输入是224x224，那么就会对图片产生变形。</p><img src="/posts/2015/05/17/cnn-trick/warp-image.png"><p>那么有没有办法使得网络可以接受任意的输入？实际上是可以的，只需要把全连接层变成卷积层，这就是所谓的卷积化。这里需要证明卷积化的等价性。直观上理解，卷积跟全连接都是一个点乘的操作，区别在于卷积是作用在一个局部的区域，而全连接是对于整个输入而言，那么只要把卷积作用的区域扩大为整个输入，那就变成全连接了，我就不给出形式化定义了。所以我们只需要把卷积核变成跟输入的一个map的大小一样就可以了，这样的话就相当于使得卷积跟全连接层的参数一样多。举个例子，比如AlexNet，fc6的输入是256x6x6，那么这时候只需要把fc6变成是卷积核为6x6的卷积层就好了。</p><p>例子：(1) 用全连接的: <a href="/posts/2015/05/17/cnn-trick/full-connected.prototxt" title="full-connected.prototxt">full-connected.prototxt</a>，(2) 改成全卷积：<a href="/posts/2015/05/17/cnn-trick/full-conv.prototxt" title="full-conv.prototxt">full-conv.prototxt</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章主要讲一下Convolutional Neural Network(CNN)里面的一些概念以及技巧。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
      <category term="Trick" scheme="http://blog.cvmarcher.cn/tags/Trick/"/>
    
  </entry>
  
  <entry>
    <title>Notes for &quot;Object detection via a multi-region &amp; semantic segmentation-aware CNN model&quot;</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/</id>
    <published>2015-05-17T03:37:12.000Z</published>
    <updated>2018-04-26T06:29:35.868Z</updated>
    
    <content type="html"><![CDATA[<p><a href="http://arxiv.org/abs/1505.01749" target="_blank" rel="noopener">paper链接</a></p><p>我觉得是一个挺有意思的paper，整体pipeline还是proposals –&gt; feature extraction –&gt; classification –&gt; post processing。</p><p>那么跟RCNN有啥区别？大体有两点：1) 不同的feature，作者加入了<strong>multi-region features</strong>以及<strong>semantic segmentation-aware features</strong>，2)不同的后处理过程，作者加入了迭代式定位以及bbox voting的方式。这个方法在PASCAL VOC2007的mAP是<strong>74.9%</strong>（RCNN是<strong>66%</strong>），在VOC2012是<strong>70.7%</strong>（RCNN是<strong>63%</strong>），相当大的提升啊。</p><a id="more"></a><p>下面就从feature extraction跟post processing来讲。</p><h1 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h1><p>旨在找到更多具有判别性的特征。</p><h2 id="Multi-Region-CNN-features"><a href="#Multi-Region-CNN-features" class="headerlink" title="Multi-Region CNN features"></a>Multi-Region CNN features</h2><p>对物体不同区域进行建模，得到具有更丰富表现性的特征。感觉就有点DPM的感觉了（不过是手动指定parts）。作者使用的区域有以下几种：</p><ol><li><em>Original candidate box</em> (a): 这个就是proposal产生的区域了，用来得到物体整体的外观信息。</li><li><em>Half boxes</em> (b/c/d/e): 把proposal按上下左右切半得到的区域。用来只学习一半物体的特征，可以用来处理遮挡情况。</li><li><em>Central Regions</em> (f/g): 这部分的regions是用来学习物体中间部分的特征。</li><li><em>Border Regions</em> (h/i): 这部分regions是用来学习物体边缘的特征，而且是joint来学。这部分特征对不精确定位的proposal的边缘特别敏感，因为不精确定位，比如过小，会导致丢失了边缘的信息。不过话说回来，这跟<strong>Half boxes</strong>不是有些矛盾么？</li><li><em>Contextual Region</em> (j): 这种region是用来学习物体周围的信息。</li></ol><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/region_samples.jpg"><p>最外围黄色的框表示实际上截取出来的region，上面的regions有些是Rectangular rings(矩形圈): g/h/i/j，在这些矩形圈里面，内圈的信息是我们不想要的，所以实际上在截取regions的时候，要简单做个后处理，就是对内圈的信息全部置为0。</p><p>这时候很多人就会想着把regions截取出来直接扔到CNN里面训练就好了，但是这会使得预算量比RCNN多了很多，速度会超级超级慢。所以作者在截取regions出来之后并不是直接用这些regions来训练，而是用它们在全图的feature maps上对应的位置截取相应的regions来训练(<a href="/2015/05/17/cnn-trick">这里</a>有讲)。这种方式有几个好处是：1) 节省计算，2) 减少参数，能起到正则化的作用，3) 能一起finetune，互相约束，我在做ICCV的时候也使用了这样的想法，果然在CNN里面大家的想法都是类似的，理论性不强，局限性很强，比的就是谁先做出来了，谁的机器多，谁的数据多。</p><p>那么就得出了下面的网络结构:</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/multi_region_cnn.jpg"><p>网络就可以分为两部分：activation maps module和region adaptation modules，region adaptation modules用了hekaiming的spatial pooling layer，能够防止了RCNN里面的强制变形，也能在一起程度上处理multi-scale（后面有时间会写一个RCNN/SPP-NET/Fast RCNN的对比）。</p><p>网络用的是VGG，在训练的时候，所有convolution layers的参数都是固定的，每个region modules各自finetune所有的全链接层。finetune方法跟各家都类似了，把1000类的softmax换成N+1类的，N是正样本的类别数，1是背景。proposal跟groundtruth的overlap大于0.5的认为是正样本，负样本的overlap在[0.1, 0.5)，不过为何是至少要有0.1的overlap呢？</p><h2 id="Semantic-Segmentation-Aware-CNN-features"><a href="#Semantic-Segmentation-Aware-CNN-features" class="headerlink" title="Semantic Segmentation-Aware CNN features"></a>Semantic Segmentation-Aware CNN features</h2><p>Motivation是分割跟检测相关性很强，是啊，我们<a href="http://vision.sysu.edu.cn/projects/deep-joint-task-learning/" target="_blank" rel="noopener">NIPS的工作</a>也这么想。这个CNN加了个Aware，顿时间觉得好高大上，名字很重要。</p><p>网络结构如下：</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/ssa_cnn.jpg"><p>同样的，网络也分成了两部分：</p><ul><li><strong><em>Activation Maps Module</em></strong>: 就是一个<a href="http://arxiv.org/abs/1411.4038" target="_blank" rel="noopener">FCN</a>，不一样的地方是他们加入了降维：先在VGG上finetune，然后把fc7从4096个channels改为512，然后再finetune一次。label的设置见图，作者为了不引入额外的anntation，他把ground truth bbox里面的像素全部标记为前景，其它的为背景。训练的时候每个类单独训练二元分类器，这样就会产生跟类别数量一样的分割图，这种做法由它设置label的方式决定的，因为物体会存在互相遮挡的情况（不同bounding box有可能存在overlap），一个proposa可能会覆盖多个物体，所以存在某个像素点可以同属于不同的类别，也就是说不能单纯的按照bounding box去设置label（这种label是指0, 1, 2, 3, 4, .., N，N表示类别数），然后只用一个softmax去做分类。举个例子讲，一个像素点属于A类跟属于B类并不是互斥的，这种情况下用2个二元分类器会比用一个有3个label的softmax更靠谱。</li></ul><ul><li><strong><em>Region Adaptation Module</em></strong>: 这个的输入是摘掉了分类层的FCN。然后训练方式就跟每个Multi-region module一样了。但是前面提到Segmentation Maps是用N个二元分类器干的，应该怎么把Activation Maps传给Region Adaptation Module？文章中没提到，我猜想是把N个二元分类器的activation maps拼接在一起然后喂到这个module里面。</li></ul><p>最终把这两个features拼在一起就组成了下面的网络：</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/whole_net.png"><h1 id="Object-Localization"><a href="#Object-Localization" class="headerlink" title="Object Localization"></a>Object Localization</h1><p>大体分为两个步骤：迭代定位(Iterative Localization)跟投票(Bounding box voting)。</p><h2 id="Iterative-Localization"><a href="#Iterative-Localization" class="headerlink" title="Iterative Localization"></a>Iterative Localization</h2><p>在给proposals打分跟对proposals的bbox进行refine这两步进行迭代。给定proposals，对proposals进行打分，根据分数对proposals进行筛选，对剩下的proposals进行打分并筛选，如此循环，直到收敛。一开始，proposals是由selective search产生，作者说迭代两次就够了。</p><h2 id="Bounding-box-voting"><a href="#Bounding-box-voting" class="headerlink" title="Bounding box voting"></a>Bounding box voting</h2><p>是一个普通nms的改进版本，普通的nms是找两个overlap大于一定阈值的proposals，然后把分数低的剔除掉，而作者使用的方法是：假设有proposals的集合为$P = \{p_1, p_2, …, p_n\}$，先找出分数最大的proposal $p_r$，然后找出跟这个proposal的overlap大于一定阈值的所有proposals $P_c$出来(假设$P_c$为空，就选次大的$p_r$，重复上面步骤)，对他们进行位置信息加权平均，权值是他们的分数（要做归一化），然后得到新的proposal $p_new$，把$p_r \cap P_c$从$P$里面删掉，把$p_new$加到$P$里面，继续上面的操作指导无法合并。</p><p>示意图如下：</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/localization.jpg"><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>作者对每个region adaptation module都做了单独的实验，单独的话效果都很一般般。</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/result_individual.jpg"><p>合起来之后，并且加上作者提到的Localization Scheme之后，效果就飞起来了:</p><p><strong>PASCAL VOC 2007</strong>:</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/result_2007.jpg"><p><strong>PASCAL VOC 2012</strong>:</p><img src="/posts/2015/05/17/multi-region-semantic-segmentation-aware-cnn/result_2012.jpg"><p>pipeline里面的每个component是分离的，每一个component的性能都会影响整体的性能，为什么没人把整个pipeline(至少我目前还没看过)？当然这有一个好处是，每个component可以单独调参数。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;http://arxiv.org/abs/1505.01749&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;paper链接&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;我觉得是一个挺有意思的paper，整体pipeline还是proposals –&amp;gt; feature extraction –&amp;gt; classification –&amp;gt; post processing。&lt;/p&gt;
&lt;p&gt;那么跟RCNN有啥区别？大体有两点：1) 不同的feature，作者加入了&lt;strong&gt;multi-region features&lt;/strong&gt;以及&lt;strong&gt;semantic segmentation-aware features&lt;/strong&gt;，2)不同的后处理过程，作者加入了迭代式定位以及bbox voting的方式。这个方法在PASCAL VOC2007的mAP是&lt;strong&gt;74.9%&lt;/strong&gt;（RCNN是&lt;strong&gt;66%&lt;/strong&gt;），在VOC2012是&lt;strong&gt;70.7%&lt;/strong&gt;（RCNN是&lt;strong&gt;63%&lt;/strong&gt;），相当大的提升啊。&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="Detection" scheme="http://blog.cvmarcher.cn/tags/Detection/"/>
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
  </entry>
  
  <entry>
    <title>记录使用Ubuntu14.04遇到的问题(长期更新)</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/14/ubuntu-14-04-problem/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/14/ubuntu-14-04-problem/</id>
    <published>2015-05-14T10:49:24.000Z</published>
    <updated>2018-04-26T06:29:35.896Z</updated>
    
    <content type="html"><![CDATA[<p>不知道为什么我用Linux的系统总是会出现各种各样的问题- -!!!<br>在这里记录一下在使用Ubuntu14.04遇到的问题，以及装各种软件的一些方法。</p><a id="more"></a><h1 id="安装wps"><a href="#安装wps" class="headerlink" title="安装wps"></a>安装wps</h1><p>官网: <a href="http://linux.wps.cn/" target="_blank" rel="noopener">http://linux.wps.cn/</a></p><p>安装前需要安装32位库，不同于以往的版本，ia32-libs已经被抛弃掉，需要安装另外的32位库：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">apt-get</span> <span class="selector-tag">install</span> <span class="selector-tag">lib32z1</span> <span class="selector-tag">lib32ncurses5</span> <span class="selector-tag">lib32bz2-1</span><span class="selector-class">.0</span> <span class="selector-tag">libfontconfig1</span><span class="selector-pseudo">:i386</span> <span class="selector-tag">libXrender1</span><span class="selector-pseudo">:i386</span> <span class="selector-tag">libsm6</span><span class="selector-pseudo">:i386</span> <span class="selector-tag">libfreetype6</span><span class="selector-pseudo">:i386</span> <span class="selector-tag">libglib2</span><span class="selector-class">.0-0</span><span class="selector-pseudo">:i386</span></span><br></pre></td></tr></table></figure><p>安装完上面的32库之后才安装wps。启动wps的时候会提示缺失字体，对应的从windows拷过来安装即可。</p><h1 id="双显卡问题"><a href="#双显卡问题" class="headerlink" title="双显卡问题"></a>双显卡问题</h1><p>之前遇到的问题：插入了telsa k40之后，电脑就无法显示了。k40是服务版的显卡，不带视频输出，而BIOS默认是使用外置显卡输出，这就导致了电脑没法显示。解决办法很简单，就是到BIOS把集成显卡（或者是可以用来显示的显卡）作为主要设备，这样就解决了输出的问题了。但是装完NVIDIA的显卡驱动之后，进入ubuntu只能看到桌面背景，除此之外什么也看不到，后来上网找到了<a href="http://osdf.github.io/blog/intel-integrated-graphics-dedicated-gpu-for-cuda-and-ubuntu-1310.html" target="_blank" rel="noopener">解决办法</a>。</p><p>本文可以说是上诉解决办法的中文翻译。这种方法是以集成显卡(Intel家的)作为显示，NVIDIA系列的显卡作为CUDA设备为例子。请<strong>严格</strong>按照下面教程操作，特别是<strong>Step 3</strong>。</p><h2 id="Step-1-BIOS"><a href="#Step-1-BIOS" class="headerlink" title="Step 1 (BIOS)"></a>Step 1 (BIOS)</h2><p>到BIOS里面把集显(integrated graphics unit, iGP)设为主要设备。</p><h2 id="Step-2-安装显卡驱动"><a href="#Step-2-安装显卡驱动" class="headerlink" title="Step 2 (安装显卡驱动)"></a>Step 2 (安装显卡驱动)</h2><h3 id="禁用跟NVIDIA驱动有冲突的驱动"><a href="#禁用跟NVIDIA驱动有冲突的驱动" class="headerlink" title="禁用跟NVIDIA驱动有冲突的驱动"></a>禁用跟NVIDIA驱动有冲突的驱动</h3><p>把下面的内容写到文件<code>/etc/modprobe.d/blacklist.conf</code>里。写完之后需要重启电脑使得blacklist生效。</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">blacklist </span>nouveau</span><br><span class="line"><span class="keyword">blacklist </span><span class="keyword">lbm-nouveau</span></span><br><span class="line"><span class="keyword">blacklist </span>amd76x_edac</span><br><span class="line"><span class="keyword">blacklist </span>vga<span class="number">16f</span>b</span><br><span class="line"><span class="keyword">blacklist </span>rivatv</span><br><span class="line"><span class="keyword">blacklist </span>rivafb</span><br><span class="line"><span class="keyword">blacklist </span>nvidiafb</span><br><span class="line"><span class="keyword">blacklist </span>nvidia-173</span><br><span class="line"><span class="keyword">blacklist </span>nvidia-96</span><br><span class="line"><span class="keyword">blacklist </span>nvidia-current</span><br><span class="line"><span class="keyword">blacklist </span>nvidia-173-updates</span><br><span class="line"><span class="keyword">blacklist </span>nvidia-96-updates</span><br><span class="line">alias nvidia nvidia_current_updates</span><br><span class="line">alias nouveau off</span><br><span class="line">alias <span class="keyword">lbm-nouveau </span>off</span><br></pre></td></tr></table></figure><h3 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h3><figure class="highlight crystal"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install freeglut3 freeglut3-dev build-essential libx11-dev libxmu-dev libxi-dev libgl1-mesa-glx libglu1-mesa libglu1-mesa-dev gcc g++ linux-headers-generic linux-source</span><br><span class="line">sudo ln -s /usr/<span class="class"><span class="keyword">lib</span>/<span class="title">x86_64</span>-<span class="title">linux</span>-<span class="title">gnu</span>/<span class="title">libglut</span>.<span class="title">so</span>.3 /<span class="title">usr</span>/<span class="title">lib</span>/<span class="title">libglut</span>.<span class="title">so</span></span></span><br></pre></td></tr></table></figure><h3 id="安装驱动"><a href="#安装驱动" class="headerlink" title="安装驱动"></a>安装驱动</h3><p>到<a href="http://www.nvidia.com/Download/index.aspx" target="_blank" rel="noopener">NVIDIA官网</a>下载驱动。然后进行安装，安装的时候先切换到文本终端(按<code>ctrl</code>+<code>alt</code>+<code>F1</code>，除了<code>F1</code>，也可以是<code>F2 ~ F6</code>任意一个)，并把图形界面给关掉。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> service </span>lightdm stop</span><br><span class="line">sudo chmod +x NVIDIA???.run</span><br><span class="line">sudo ./NVIDIA???.run</span><br></pre></td></tr></table></figure><p>接下来把NVIDIA模块加到Linux内核里面</p><figure class="highlight ebnf"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo modprobe nvidia-uvm</span></span><br></pre></td></tr></table></figure><p>然后使用命令<code>nvidia-smi</code>看驱动是否安装成功。</p><h2 id="Step-3-重装Intel显卡驱动"><a href="#Step-3-重装Intel显卡驱动" class="headerlink" title="Step 3 (重装Intel显卡驱动)"></a>Step 3 (重装Intel显卡驱动)</h2><p>这步是最最最最关键的。装了显卡驱动之后，如果直接启动图形界面，会发现只能看到桌面背景，这是因为OpenGL库会被NVIDIA的覆盖掉，我们需要重新安装：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get <span class="keyword">install </span>--reinstall xserver-<span class="keyword">xorg-core </span> xserver-<span class="keyword">xorg-video-intel </span>xserver-<span class="keyword">xorg-video-glamoregl </span>libgl1-mesa-glx</span><br></pre></td></tr></table></figure><p>装完之后就可以打开图形界面了</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> service </span>lightdm start</span><br></pre></td></tr></table></figure><h2 id="Step-4-安装CUDA"><a href="#Step-4-安装CUDA" class="headerlink" title="Step 4 (安装CUDA)"></a>Step 4 (安装CUDA)</h2><p>接下来就是安装CUDA了，到<a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">官网</a>下载。因为我用的都是CUDA-6.5及之前的版本，这个版本是需要使用gcc 4.6版本的，所以得先安装gcc 4.6。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">apt-get</span> <span class="selector-tag">install</span> <span class="selector-tag">gcc-4</span><span class="selector-class">.6</span> <span class="selector-tag">g</span>++<span class="selector-tag">-4</span><span class="selector-class">.6</span></span><br></pre></td></tr></table></figure><p>然后需要让系统把gcc链接到gcc-4.6上。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo update-alternatives <span class="params">--remove-all</span> gcc</span><br><span class="line">sudo update-alternatives <span class="params">--config</span> gcc</span><br><span class="line">sudo update-alternatives <span class="params">--install</span> <span class="string">/usr/bin/gcc</span> gcc <span class="string">/usr/bin/gcc-4.6</span> 10</span><br><span class="line">sudo update-alternatives <span class="params">--install</span> <span class="string">/usr/bin/gcc</span> gcc <span class="string">/usr/bin/gcc-4.8</span> 50</span><br><span class="line">sudo update-alternatives <span class="params">--config</span> gcc          <span class="comment">#选择 4.6</span></span><br></pre></td></tr></table></figure><p>然后就直接安装CUDA了。注意在安装的时候会提示你是否安装NVIDIA的显卡驱动，这时候不需要安装NVIDIA显卡驱动，只需要安装CUDA-Toolkit就行。</p><figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo chmod +x cuda_6.5???<span class="string">.run</span></span><br><span class="line">sudo <span class="string">./cuda_</span>???<span class="string">.run</span></span><br></pre></td></tr></table></figure><p>安装完之后就把gcc重新链接到gcc-4.8上。</p><figure class="highlight dsconfig"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">sudo </span><span class="string">update-alternatives </span><span class="built_in">--config</span> <span class="string">gcc </span>         <span class="comment">#选择 4.8</span></span><br></pre></td></tr></table></figure><p>然后把环境变量配好(加到<code>~/.bashrc</code>即可)。</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="builtin-name">export</span> <span class="attribute">PATH</span>=/usr/local/cuda/bin:$PATH</span><br><span class="line"><span class="builtin-name">export</span> <span class="attribute">LD_LIBRARY_PATH</span>=/usr/local/cuda/lib64:$LD_LIBRARY_PATH</span><br></pre></td></tr></table></figure><p>然后再测试一下安装是否成果，跑去他自带的Sample里面随便找个运行一下就好了。</p><p>大功告成，show一下我的显卡: Telsa K40c + GTX TiTan Black，其实还有的，只是我的主板比较弱，只有两个插槽….</p><img src="/posts/2015/05/14/ubuntu-14-04-problem/nvidia-smi.png">]]></content>
    
    <summary type="html">
    
      &lt;p&gt;不知道为什么我用Linux的系统总是会出现各种各样的问题- -!!!&lt;br&gt;在这里记录一下在使用Ubuntu14.04遇到的问题，以及装各种软件的一些方法。&lt;/p&gt;
    
    </summary>
    
      <category term="折腾" scheme="http://blog.cvmarcher.cn/categories/%E6%8A%98%E8%85%BE/"/>
    
    
      <category term="Ubuntu" scheme="http://blog.cvmarcher.cn/tags/Ubuntu/"/>
    
      <category term="Nvidia" scheme="http://blog.cvmarcher.cn/tags/Nvidia/"/>
    
      <category term="双显卡" scheme="http://blog.cvmarcher.cn/tags/%E5%8F%8C%E6%98%BE%E5%8D%A1/"/>
    
  </entry>
  
  <entry>
    <title>Action Classification In Still Image (CNN篇)</title>
    <link href="http://blog.cvmarcher.cn/posts/2015/05/13/action-still-image-cnn/"/>
    <id>http://blog.cvmarcher.cn/posts/2015/05/13/action-still-image-cnn/</id>
    <published>2015-05-13T07:45:06.000Z</published>
    <updated>2018-04-26T06:29:35.856Z</updated>
    
    <content type="html"><![CDATA[<p>赶完ICCV之后，导师还是觉得需要把之前<a href="http://hcp.sysu.edu.cn/an-expressive-deep-model-for-human-action-parsing-from-a-single-image/" target="_blank" rel="noopener">ICME的工作</a>扩展成期刊(当时拿了Best Student Paper的时候已经叫了。。过了快一年了，拖延症啊)，于是开始重新涉猎静态图片下的行为识别，关注在Deep Learning方面的paper，迟点有空会写一下传统的方法。</p><p>大概搜了一下，不得不说，<a href="http://www.cs.berkeley.edu/~gkioxari/" target="_blank" rel="noopener">Georgia Gkioxari</a>这个美女发了几篇了…</p><a id="more"></a><h3 id="Learning-and-Transferring-Mid-Level-Image-Representations-using-Convolutional-Neural-Networks"><a href="#Learning-and-Transferring-Mid-Level-Image-Representations-using-Convolutional-Neural-Networks" class="headerlink" title="Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks"></a>Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks</h3><p><a href="http://lear.inrialpes.fr/workshop/allegro/slides/oquab.pdf" target="_blank" rel="noopener">paper链接</a></p><p>CVPR 2014的paper，文章核心是Transfer Learning，把在大规模的数据集下得到的模型的部分参数迁移到新模型上，一般这方面的工作都是基于在imagenet上训练的模型（AlexNet, VGG），为啥这么做？一般都认为大部分模型前面几层学习的都是一些很普通的特征（如gabor，有人可视化过），后面几层是跟任务相关的，所以通常的做法就是：</p><ol><li>先在数据量大的数据集上训练一个模型。</li><li>把这个模型的后面几层删掉，换成新的层。</li><li>固定没有被换掉的层的参数，新模型在目标数据集上进行finetune。<br>如下图，这篇paper用的AlexNet，把FC8去掉，重新加了两个全链接，在要测试的数据集上进行finetune。在PASCAL 2007/2012上物体分类以及Pascal 2012动作分类上都取得很好效果。</li></ol><img src="/posts/2015/05/13/action-still-image-cnn/Learning_and_Transferring_Mid-Level_Image_Representations_using_Convolutional_Neural_Networks-net.png"><p>文章另外两个值得学习的部分是：<br>[1] 如何对样本进行分类（设置groundtruth）。会从两个方面考虑(1)正样本的bbox( $B_o$ )跟patch( $P$ )的重合程度，$|P \cap B_o| \ge 0.2|P| \&amp;\&amp;  |P \cap B_o| \ge 0.6|B_o|$, (2)一个被当做正样本的patch不能包含两个object。<br>[2] 样本均衡问题。文章的做法是从background patches里面选取10%用来训练，其实我觉得没必要这么做，使用SGD来训练的时候，只需要调整每个mini-batch的正负样本即可，比如一个batch大小是10，可以3个是正样本，7个是负样本，这个方法实质是使得每次更新参数的时候，负样本对权值的影响不会是压倒性的。这个方法我屡试不爽。</p><p>在生成样本的时候，每张图片大概会在8个尺度下随机产生500个方形patches，而且这些patches里面，相邻的patches需要有50% overlap，测试也是每张图片用500个patch，然后通过下面的公式进行融合，$C_n$是类别，$M$是patches数量（500），$y(C_n | P_i)$就是每个patch对应的概率输出，$k$是可调参数，$k$越大表示越关心概率大的patch：</p><p>$$ score(C_n) = \frac{1}{M}y(C_n | P_i)^k $$</p><p>贴一下样本分类的图：</p><img src="/posts/2015/05/13/action-still-image-cnn/Learning_and_Transferring_Mid-Level_Image_Representations_using_Convolutional_Neural_Networks-samples.jpg"><h3 id="Action-and-Attributes-from-Wholes-and-Parts"><a href="#Action-and-Attributes-from-Wholes-and-Parts" class="headerlink" title="Action and Attributes from Wholes and Parts"></a>Action and Attributes from Wholes and Parts</h3><p><a href="http://www.cs.berkeley.edu/~gkioxari/deepparts.pdf" target="_blank" rel="noopener">paper链接</a></p><p>我觉得这个paper写得很难懂….</p><p>思路是用CNN做分类，如下图，先通过把人体不同的part检测出来，然后把对每个part提取AlexNet（CNN）的fc7的features，把这些features拼接起来得到concat features，然后把这个concat features作为线性svm的输入进行训练。</p><img src="/posts/2015/05/13/action-still-image-cnn/Action_and_Attributes_from_Wholes_and_Parts-net.png"><p>然后文章的贡献就是如果检测part，因为还是走的是feature pyramid + sliding windows + classification流程，那么就必须得先训练好单一尺度的part的分类器。文章并不是对人的每个关键点训练一个分类器，而是定义了所谓的high-level part：头/躯干/腿，这相当于把整个人从上到下分成了三部分，下图是例子。</p><img src="/posts/2015/05/13/action-still-image-cnn/Action_and_Attributes_from_Wholes_and_Parts-parts-samples.png"><p>对于每个part，内部还会分不同的类型，比如头部，就可能会被分成左侧脸/右侧脸/正脸，当然这只是我直观上的说法，实际上文章是通过聚类得到的。这跟poselet很像，所以作者也说自己是deep poselet…</p><p>同样的，他训练part的分类器跟训练action的分类器类似，也是提取AlexNet某层的feature出来，然后把他扔到线性svm里面训练。在最终测试的时候，part检测是会在给定bbox（可以是groundtruth，可以是检测的结果，例如用R-CNN来检测）的情况下进行检测，对于每个part，会选择在给定bbox下的最大响应作为输出。</p><h3 id="R-CNNs-for-Pose-Estimation-and-Action-Detection"><a href="#R-CNNs-for-Pose-Estimation-and-Action-Detection" class="headerlink" title="R-CNNs for Pose Estimation and Action Detection"></a>R-CNNs for Pose Estimation and Action Detection</h3><p><a href="http://www.cs.berkeley.edu/~gkioxari/multiloss.pdf" target="_blank" rel="noopener">paper链接</a></p><p>这个paper比较简单，一个简单的multi-task paper。网络结构如下：</p><img src="/posts/2015/05/13/action-still-image-cnn/R-CNNs_for_Pose_Estimation_and_Action_Detection-net.png"><p>总共有3个输出，分别是人体检测/姿势估计/动作分类，具体的loss看paper就好了，最终这三个loss是加权起来，通过控制权值可以实现单任务训练以及多任务训练。</p><p>跟R-CNN类似，这个网络的输入是object proposal。</p><h3 id="Contextual-Action-Recognition-with-R-CNN"><a href="#Contextual-Action-Recognition-with-R-CNN" class="headerlink" title="Contextual Action Recognition with R*CNN"></a>Contextual Action Recognition with R*CNN</h3><p><a href="http://www.cs.berkeley.edu/~gkioxari/frstarcnn.pdf" target="_blank" rel="noopener">paper链接</a></p><p>这篇paper是结合了context信息来做，这么看来感觉已经追上了传统的步伐了…套路是：只对整个人建模 –&gt; 对part也建模 –&gt; 对context信息建模，接下来就是对part/context/object同时建模了。</p><p>这个paper很简单，可以说是传统的方法换了个特征。尽管如何，还是有点创意的，创意在于作者怎么去对context建模，也就是如何找对行为分类有用的context区域。作者提出了一个R*CNN的方法，他在这个方法里面指定了两个区域，一个叫primary region，这个区域是包括一个人，由ground truth box给出，这个人的动作需要被分类；另外一个叫secondary region，这个区域包含的是对行为分类有用的context信息。secondary regions是由一些region proposal(论文用的是selective search)的方法产生，因为是用region proposal的方法产生，所以可能会存在多个region，那么就存在如何选一个最有效的region出来的问题。</p><p>假设$\phi$表示特征，$r$表示primary region，$R(r; I)$表示secondary regions的候选区域，其中<span>$R(r; I) = \{s \in S(I); overlap(s, r) \in [l, u]$</span><!-- Has MathJax -->，$s$表示secondary region，上面的条件约束了$s$的候选区域，让$s$跟$r$的交并比在一定范围内。<span>$s^*$</span><!-- Has MathJax -->表示最好的secondary region，那么<br>$$<br>\begin{equation}<br>s^* = arg\max_{s \in R(r; I)}w_s^{\alpha} \cdot \phi(s; I)<br>\end{equation}<br>$$</p><p>最终每个action的score：</p><span>$$\begin{equation}score(\alpha; I, r) = w_p^{\alpha} \cdot \phi(r; I) + \max_{s \in R(r; I)}w_s^{\alpha} \cdot \phi(s; I)\end{equation}$$</span><!-- Has MathJax --><p>接着用个softmax转换为概率的形式：</p><span>$$\begin{equation}P(\alpha | I, r) = \frac{exp(score(\alpha'; I, r)}{\sum{exp(score(\alpha'; I, r)}}\end{equation}$$</span><!-- Has MathJax --><p>最终上面的公式都可以映射到网络里面，网络结构如下：</p><img src="/posts/2015/05/13/action-still-image-cnn/Contextual_Action_Recognition_with_R-start-CNN-net.png"><p>作者用的是VGG net，在conv5出来之后就会分两路，红色那路是用来处理primary region，绿色那路是用来处理secondary regions，绿色那里的secondary regions的数量是固定的，每次随机的从$R(r; I)$里面选N个，最终在secondary regions选取一个score最大的跟primary region的score相加，然后经过softmax。实际训练中，fc6/fc7是不动的，finetune的是score这一层的参数。</p><p>大概思路就是这样。</p><p>然后看一下网络选取secondary regions的情况：</p><img src="/posts/2015/05/13/action-still-image-cnn/region-proposals.png"><p>其实可以隐约的发现，secondary regions都是跟action比较相关的，所以说secondary regions还是比较靠谱的，其实这跟Vittorio在<a href="http://www.vision.ee.ethz.ch/publications/get_abstract.cgi?articles=828&amp;mode=&amp;lang=en" target="_blank" rel="noopener">12年的PAMI</a>的想法类似的，不过显然，这个模型远不够Vittorio的好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;赶完ICCV之后，导师还是觉得需要把之前&lt;a href=&quot;http://hcp.sysu.edu.cn/an-expressive-deep-model-for-human-action-parsing-from-a-single-image/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;ICME的工作&lt;/a&gt;扩展成期刊(当时拿了Best Student Paper的时候已经叫了。。过了快一年了，拖延症啊)，于是开始重新涉猎静态图片下的行为识别，关注在Deep Learning方面的paper，迟点有空会写一下传统的方法。&lt;/p&gt;
&lt;p&gt;大概搜了一下，不得不说，&lt;a href=&quot;http://www.cs.berkeley.edu/~gkioxari/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Georgia Gkioxari&lt;/a&gt;这个美女发了几篇了…&lt;/p&gt;
    
    </summary>
    
      <category term="Academic" scheme="http://blog.cvmarcher.cn/categories/Academic/"/>
    
    
      <category term="CNN" scheme="http://blog.cvmarcher.cn/tags/CNN/"/>
    
      <category term="Action" scheme="http://blog.cvmarcher.cn/tags/Action/"/>
    
      <category term="Still Image" scheme="http://blog.cvmarcher.cn/tags/Still-Image/"/>
    
  </entry>
  
</feed>
